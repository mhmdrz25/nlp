{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "H7vI0cF09bln",
        "outputId": "31c5d23c-ef4b-465c-ee03-087b228f7496"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bfa1c54d-127d-4e40-a88d-ee8763bd1a29\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bfa1c54d-127d-4e40-a88d-ee8763bd1a29\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving dreaddit-test (3).csv to dreaddit-test (3).csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "ODOa4n-z9mvM",
        "outputId": "46fdd8db-019e-495d-aeeb-26f2067064ed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0a5bf17e-3094-4f06-a63e-6c73c7dc0300\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0a5bf17e-3094-4f06-a63e-6c73c7dc0300\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving dreaddit-train.csv to dreaddit-train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hunggingface_hub['hf_xet']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShsfSOG1A1tv",
        "outputId": "06e310ee-75d0-41a1-a4b7-b8e9ff1d06cf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement hunggingface_hub[hf_xet] (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for hunggingface_hub[hf_xet]\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub[hf_xet]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-EYZXh1BEU2",
        "outputId": "0f2d06e8-fccb-4e97-acd1-7b83073e95ff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub[hf_xet] in /usr/local/lib/python3.11/dist-packages (0.31.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (4.13.2)\n",
            "Collecting hf-xet<2.0.0,>=1.1.1 (from huggingface_hub[hf_xet])\n",
            "  Downloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (2025.4.26)\n",
            "Downloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hf-xet\n",
            "Successfully installed hf-xet-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import copy\n",
        "import json\n",
        "import logging\n",
        "import random\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import seaborn as sns\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModel, AutoConfig,\n",
        "    TrainingArguments, Trainer, EarlyStoppingCallback,\n",
        "    DistilBertTokenizer, DistilBertModel,\n",
        "    get_cosine_schedule_with_warmup\n",
        "\n",
        ")\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import (\n",
        "    f1_score, accuracy_score, confusion_matrix, classification_report,\n",
        "    precision_score, recall_score, roc_auc_score\n",
        ")\n",
        "from nltk.corpus import wordnet\n",
        "import nltk\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from tqdm import tqdm\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from transformers import pipeline, BertForMaskedLM\n",
        "\n",
        "import psutil\n",
        "import time\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "7oHCCab-BHsw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "# ArgParse setup\n",
        "import argparse\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--seed\", type=int, default=42)\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=32)\n",
        "    parser.add_argument(\"--max_length\", type=int, default=128)\n",
        "    parser.add_argument(\"--num_epochs\", type=int, default=2)\n",
        "    args, unknown = parser.parse_known_args()  # Ignore unrecognized arguments\n",
        "    return args\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Set up NLTK\n",
        "nltk_data_path = os.path.join(os.getcwd(), 'nltk_data')\n",
        "os.makedirs(nltk_data_path, exist_ok=True)\n",
        "nltk.data.path.append(nltk_data_path)\n",
        "try:\n",
        "    nltk.download('wordnet', download_dir=nltk_data_path, quiet=True)\n",
        "except Exception as e:\n",
        "    logger.warning(f\"NLTK download warning: {str(e)}\")\n",
        "\n",
        "# Set seed for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Performance optimizations for PyTorch\n",
        "torch.backends.cudnn.benchmark = True  # Speed up on GPU with variable input sizes\n",
        "torch.backends.cudnn.deterministic = False  # Increase performance at the cost of reproducibility"
      ],
      "metadata": {
        "id": "TY_sFL_mBKN1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvancedDataAugmenter:\n",
        "    def __init__(self, alpha_sr=0.05, alpha_rd=0.05, alpha_ri=0.05, alpha_rs=0.05, num_aug=2):\n",
        "        self.alpha_sr = alpha_sr\n",
        "        self.alpha_rd = alpha_rd\n",
        "        self.alpha_ri = alpha_ri\n",
        "        self.alpha_rs = alpha_rs\n",
        "        self.num_aug = num_aug\n",
        "\n",
        "        # تنظیم GPU/CPU برای مدل‌های Transformer\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        # Using Hugging Face pipeline for text generation (BERT-like model)\n",
        "        device_id = 0 if self.device == 'cuda' else -1\n",
        "        self.contextual_aug = pipeline(\n",
        "            \"fill-mask\", model=\"bert-base-uncased\", device=device_id\n",
        "        )\n",
        "\n",
        "    def get_synonyms(self, word):\n",
        "        synonyms = set()\n",
        "        try:\n",
        "            for syn in wordnet.synsets(word):\n",
        "                for lemma in syn.lemmas():\n",
        "                    synonym = lemma.name().replace(\"_\", \" \").lower()\n",
        "                    if synonym != word and len(synonym.split()) == 1:\n",
        "                        synonyms.add(synonym)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error getting synonyms for word {word}: {str(e)}\")\n",
        "        return list(synonyms)\n",
        "\n",
        "    def contextual_augmentation(self, text):\n",
        "        words = text.split()\n",
        "        if len(words) < 3:  # جلوگیری از مشکل در جملات خیلی کوتاه\n",
        "            return text\n",
        "\n",
        "        # انتخاب یک کلمه تصادفی برای جایگذاری\n",
        "        idx = random.randint(0, len(words) - 1)\n",
        "        masked_text = words.copy()\n",
        "        masked_text[idx] = \"[MASK]\"\n",
        "        masked_text = \" \".join(masked_text)\n",
        "\n",
        "        # گرفتن جایگزین پیشنهادی از BERT\n",
        "        predictions = self.contextual_aug(masked_text)\n",
        "        if predictions and len(predictions) > 0:\n",
        "            new_word = predictions[0][\"token_str\"]\n",
        "            words[idx] = new_word  # جایگذاری در متن اصلی\n",
        "        return \" \".join(words)\n",
        "    def augment_text(self, text, mode=None):\n",
        "        if not isinstance(text, str) or len(text.strip()) == 0:\n",
        "            return [text]\n",
        "\n",
        "        words = text.split()\n",
        "        if len(words) == 0:\n",
        "            return [text]\n",
        "\n",
        "        augmented_texts = []\n",
        "\n",
        "        try:\n",
        "            if mode == \"train\":  # همه عملیات افزایش داده فقط در حالت train اجرا می‌شوند\n",
        "                # Contextual augmentation\n",
        "                aug_text = self.contextual_augmentation(text)\n",
        "                if aug_text and len(aug_text.strip()) > 0:\n",
        "                    augmented_texts.append(aug_text)\n",
        "\n",
        "                # Synonym replacement\n",
        "                for _ in range(self.num_aug):\n",
        "                    new_words = words.copy()\n",
        "                    n_words = len(new_words)\n",
        "                    n_to_replace = max(1, int(self.alpha_sr * n_words))\n",
        "\n",
        "                    for _ in range(n_to_replace):\n",
        "                        idx = random.randint(0, len(new_words) - 1)\n",
        "                        synonyms = self.get_synonyms(new_words[idx])\n",
        "                        if synonyms:\n",
        "                            new_words[idx] = random.choice(synonyms)\n",
        "\n",
        "                    aug_text = ' '.join(new_words)\n",
        "                    if len(aug_text) > 10:\n",
        "                        augmented_texts.append(aug_text)\n",
        "\n",
        "                # Random word deletion\n",
        "                new_words = self.random_deletion(words, self.alpha_rd)\n",
        "                aug_text = ' '.join(new_words)\n",
        "                if len(aug_text) > 10:\n",
        "                    augmented_texts.append(aug_text)\n",
        "\n",
        "                # Random word swap\n",
        "                new_words = self.random_swap(words, int(self.alpha_rs * len(words)))\n",
        "                aug_text = ' '.join(new_words)\n",
        "                if len(aug_text) > 10 and len(new_words) > 0:\n",
        "                    augmented_texts.append(aug_text)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error during augmentation: {str(e)}\")\n",
        "\n",
        "        # اگر هیچ متن افزوده‌ای ایجاد نشد، متن اصلی را برگردان\n",
        "        return augmented_texts if augmented_texts else [text]\n",
        "\n",
        "    # توابع حذف و جابجایی تصادفی کلمات\n",
        "    def random_deletion(self, words, p=0.1):\n",
        "        # اطمینان از اینکه حداقل یک کلمه باقی می‌ماند\n",
        "        if len(words) <= 1:\n",
        "            return words.copy()\n",
        "        return [word for word in words if random.uniform(0, 1) > p]\n",
        "\n",
        "    def random_swap(self, words, n=1):\n",
        "        if len(words) < 2:\n",
        "            return words.copy()\n",
        "\n",
        "        words = words.copy()\n",
        "        n = min(n, len(words) - 1)  # حداکثر تعداد جابجایی‌ها\n",
        "\n",
        "        for _ in range(n):\n",
        "            idx1, idx2 = random.sample(range(len(words)), 2)\n",
        "            words[idx1], words[idx2] = words[idx2], words[idx1]\n",
        "        return words"
      ],
      "metadata": {
        "id": "pbjdgp9tChAw"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import DistilBertModel\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class MaxPoolCNNModel(nn.Module):\n",
        "    def __init__(self, config, max_length=128, train_labels=None):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.max_length = max_length\n",
        "        self.embedding_dim = config.hidden_size\n",
        "\n",
        "        # Load pre-trained DistilBERT\n",
        "        self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.bert.gradient_checkpointing_enable()\n",
        "\n",
        "        # CNN layers\n",
        "        self.conv1 = nn.Conv1d(self.embedding_dim, 256, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(256, 128, kernel_size=3, padding=1)\n",
        "        nn.init.kaiming_normal_(self.conv1.weight, nonlinearity='relu')\n",
        "        nn.init.kaiming_normal_(self.conv2.weight, nonlinearity='relu')\n",
        "\n",
        "        # Pooling and batch normalization\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.batch_norm1 = nn.BatchNorm1d(256)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(128)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "        # Classifier\n",
        "        pooled_output_size = 128 * (max_length // 2 // 2)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(pooled_output_size, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, 2)\n",
        "        )\n",
        "        for layer in self.classifier:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                nn.init.xavier_normal_(layer.weight)\n",
        "\n",
        "        # Class weights\n",
        "        self.class_weights = None\n",
        "        if train_labels is not None:\n",
        "            self.compute_class_weights(train_labels)\n",
        "\n",
        "        self.to(self.device)\n",
        "\n",
        "    def compute_class_weights(self, train_labels):\n",
        "        try:\n",
        "            if isinstance(train_labels, (np.ndarray, torch.Tensor)):\n",
        "                train_labels = train_labels.tolist()\n",
        "            train_labels = [label for label in train_labels if label is not None]\n",
        "            if len(np.unique(train_labels)) > 1:\n",
        "                class_weights = compute_class_weight(\n",
        "                    class_weight=\"balanced\",\n",
        "                    classes=np.unique(train_labels),\n",
        "                    y=train_labels\n",
        "                )\n",
        "                self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.device)\n",
        "                logger.info(f\"Class weights: {self.class_weights}\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error computing class weights: {e}\")\n",
        "            self.class_weights = None\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        input_ids = input_ids.to(self.device)\n",
        "        attention_mask = attention_mask.to(self.device)\n",
        "        if labels is not None:\n",
        "            labels = labels.to(self.device)\n",
        "\n",
        "        with torch.amp.autocast(device_type='cuda' if self.device.type == 'cuda' else 'cpu'):\n",
        "            bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            sequence_output = bert_output.last_hidden_state  # (batch, seq_len, hidden)\n",
        "\n",
        "            x = sequence_output.transpose(1, 2)  # (batch, hidden, seq_len)\n",
        "            x = self.conv1(x)\n",
        "            x = self.batch_norm1(x)\n",
        "            x = F.relu(x)\n",
        "            x = self.pool(x)\n",
        "\n",
        "            x = self.conv2(x)\n",
        "            x = self.batch_norm2(x)\n",
        "            x = F.relu(x)\n",
        "            x = self.pool(x)\n",
        "\n",
        "            x = x.view(x.size(0), -1)  # Flatten\n",
        "            x = self.dropout(x)\n",
        "            logits = self.classifier(x)\n",
        "\n",
        "        result = {'logits': logits}\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)\n",
        "            loss = loss_fct(logits, labels)\n",
        "            result['loss'] = loss\n",
        "\n",
        "        return result\n"
      ],
      "metadata": {
        "id": "NPawsg05CxAL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import DistilBertModel\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class GlobalPoolCNNModel(nn.Module):\n",
        "    def __init__(self, config, train_labels=None):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.embedding_dim = config.hidden_size\n",
        "\n",
        "        # Load pre-trained DistilBERT\n",
        "        self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.bert.gradient_checkpointing_enable()\n",
        "\n",
        "        # CNN layers\n",
        "        self.conv1 = nn.Conv1d(self.embedding_dim, 128, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(128, 64, kernel_size=3, padding=1)\n",
        "\n",
        "        # Residual connection\n",
        "        self.use_residual = True\n",
        "        self.projection = nn.Conv1d(self.embedding_dim, 128, kernel_size=1) if self.use_residual else None\n",
        "        nn.init.kaiming_normal_(self.conv1.weight, nonlinearity='relu')\n",
        "        nn.init.kaiming_normal_(self.conv2.weight, nonlinearity='relu')\n",
        "        if self.projection:\n",
        "            nn.init.kaiming_normal_(self.projection.weight, nonlinearity='relu')\n",
        "\n",
        "        # Global pooling\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.global_max_pool = nn.AdaptiveMaxPool1d(1)\n",
        "\n",
        "        # Batch normalization and dropout\n",
        "        self.batch_norm1 = nn.BatchNorm1d(128)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(64)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(64 * 2, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, 2)\n",
        "        )\n",
        "        for layer in self.classifier:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                nn.init.xavier_normal_(layer.weight)\n",
        "\n",
        "        # Class weights\n",
        "        self.class_weights = None\n",
        "        if train_labels is not None:\n",
        "            self.compute_class_weights(train_labels)\n",
        "\n",
        "        self.to(self.device)\n",
        "\n",
        "    def compute_class_weights(self, train_labels):\n",
        "        try:\n",
        "            if isinstance(train_labels, (np.ndarray, torch.Tensor)):\n",
        "                train_labels = train_labels.tolist()\n",
        "            train_labels = [label for label in train_labels if label is not None]\n",
        "            if len(np.unique(train_labels)) > 1:\n",
        "                class_weights = compute_class_weight(\n",
        "                    class_weight=\"balanced\",\n",
        "                    classes=np.unique(train_labels),\n",
        "                    y=train_labels\n",
        "                )\n",
        "                self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.device)\n",
        "                logger.info(f\"Class weights: {self.class_weights}\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error computing class weights: {e}\")\n",
        "            self.class_weights = None\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        input_ids = input_ids.to(self.device)\n",
        "        attention_mask = attention_mask.to(self.device)\n",
        "        if labels is not None:\n",
        "            labels = labels.to(self.device)\n",
        "\n",
        "        with torch.amp.autocast(device_type='cuda' if self.device.type == 'cuda' else 'cpu'):\n",
        "            bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            sequence_output = bert_output.last_hidden_state  # (batch, seq_len, hidden)\n",
        "\n",
        "            x = sequence_output.transpose(1, 2)  # (batch, hidden, seq_len)\n",
        "            conv1_out = self.conv1(x)\n",
        "            conv1_out = self.batch_norm1(conv1_out)\n",
        "            if self.use_residual and self.projection:\n",
        "                residual = self.projection(x)\n",
        "                conv1_out = conv1_out + residual\n",
        "            conv1_out = F.relu(conv1_out)\n",
        "\n",
        "            x = self.conv2(conv1_out)\n",
        "            x = self.batch_norm2(x)\n",
        "            x = F.relu(x)\n",
        "\n",
        "            avg_pooled = self.global_avg_pool(x).squeeze(-1)  # (batch, 64)\n",
        "            max_pooled = self.global_max_pool(x).squeeze(-1)  # (batch, 64)\n",
        "            x = torch.cat([avg_pooled, max_pooled], dim=1)\n",
        "\n",
        "            x = self.dropout(x)\n",
        "            logits = self.classifier(x)\n",
        "\n",
        "        result = {'logits': logits}\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)\n",
        "            loss = loss_fct(logits, labels)\n",
        "            result['loss'] = loss\n",
        "\n",
        "        return result\n"
      ],
      "metadata": {
        "id": "QkurCEs7C93Z"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import wandb\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import (\n",
        "    f1_score, accuracy_score, confusion_matrix, classification_report,\n",
        "    precision_score, recall_score, roc_auc_score\n",
        ")\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "\n",
        "class ModelComparison:\n",
        "    def __init__(self, model_maxpool, model_globalpool, train_dataset, val_dataset, test_dataset, batch_size=16):\n",
        "        \"\"\"\n",
        "        Initialize the ModelComparison class for training and comparing two models.\n",
        "\n",
        "        Args:\n",
        "            model_maxpool: MaxPoolCNNModel instance.\n",
        "            model_globalpool: GlobalPoolCNNModel instance.\n",
        "            train_dataset: Training dataset.\n",
        "            val_dataset: Validation dataset.\n",
        "            test_dataset: Test dataset.\n",
        "            batch_size: Batch size for data loaders.\n",
        "        \"\"\"\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Set up models\n",
        "        self.model_maxpool = model_maxpool.to(self.device)\n",
        "        self.model_globalpool = model_globalpool.to(self.device)\n",
        "\n",
        "        # Enable PyTorch 2.0+ optimizations if available\n",
        "        if hasattr(torch, 'compile'):\n",
        "            self.model_maxpool = torch.compile(self.model_maxpool)\n",
        "            self.model_globalpool = torch.compile(self.model_globalpool)\n",
        "\n",
        "        # Mixed precision training\n",
        "        self.scaler = torch.amp.GradScaler()\n",
        "\n",
        "        # Batch size and gradient accumulation\n",
        "        self.batch_size = batch_size\n",
        "        self.accumulate_grad_batches = 4\n",
        "\n",
        "        # Data loaders with optimized settings\n",
        "        self.train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=2,\n",
        "            persistent_workers=True,\n",
        "            pin_memory=True,\n",
        "            drop_last=True\n",
        "        )\n",
        "        self.val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=4,\n",
        "            persistent_workers=True,\n",
        "            pin_memory=True\n",
        "        )\n",
        "        self.test_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=4,\n",
        "            persistent_workers=True,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        # Optimizers\n",
        "        self.optimizer = {\n",
        "            'MaxPool': torch.optim.AdamW(self.model_maxpool.parameters(), lr=3e-5, weight_decay=1e-4),\n",
        "            'GlobalPool': torch.optim.AdamW(self.model_globalpool.parameters(), lr=3e-5, weight_decay=1e-4)\n",
        "        }\n",
        "\n",
        "        # Learning rate schedulers with cosine annealing and warm restarts\n",
        "        total_steps = len(self.train_loader) * 5  # 10 epochs\n",
        "        warmup_steps = int(0.1 * total_steps)\n",
        "        self.scheduler = {\n",
        "            'MaxPool': torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "                self.optimizer['MaxPool'], T_0=10, T_mult=2, eta_min=1e-6\n",
        "            ),\n",
        "            'GlobalPool': torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "                self.optimizer['GlobalPool'], T_0=10, T_mult=2, eta_min=1e-6\n",
        "            )\n",
        "        }\n",
        "\n",
        "        # Logging setup\n",
        "        self.writer = SummaryWriter('runs/anxiety_detection_experiment')\n",
        "        wandb.init(project=\"anxiety-detection\", config={\n",
        "            \"batch_size\": batch_size,\n",
        "            \"learning_rate\": 3e-5,\n",
        "            \"architecture\": \"MaxPool_vs_GlobalPool\"\n",
        "        })\n",
        "\n",
        "        # Compute class weights for the entire dataset\n",
        "        all_labels = []\n",
        "        for batch in self.train_loader:\n",
        "            all_labels.extend(batch['labels'].cpu().numpy())\n",
        "        self.class_weights = compute_class_weight(\n",
        "            class_weight=\"balanced\",\n",
        "            classes=np.unique(all_labels),\n",
        "            y=all_labels\n",
        "        )\n",
        "        self.class_weights = torch.tensor(self.class_weights, dtype=torch.float32).to(self.device)\n",
        "\n",
        "        # Loss function with class weights\n",
        "        self.num_epochs = 5  # Increased to 10 epochs\n",
        "        self.criterion = nn.CrossEntropyLoss(weight=self.class_weights)\n",
        "\n",
        "        # Initialize results dictionary\n",
        "        self.results = {\n",
        "            'MaxPool': {'train_loss': [], 'val_loss': [], 'val_f1': [], 'val_accuracy': []},\n",
        "            'GlobalPool': {'train_loss': [], 'val_loss': [], 'val_f1': [], 'val_accuracy': []}\n",
        "        }\n",
        "\n",
        "    def train_epoch(self, model, optimizer, train_loader, accumulate_grad_batches, epoch):\n",
        "        \"\"\"\n",
        "        Train the model for one epoch.\n",
        "\n",
        "        Args:\n",
        "            model: The model to train.\n",
        "            optimizer: The optimizer to use.\n",
        "            train_loader: DataLoader for the training data.\n",
        "            accumulate_grad_batches: Number of batches to accumulate gradients before updating weights.\n",
        "            epoch: Current epoch number.\n",
        "\n",
        "        Returns:\n",
        "            Average training loss for the epoch.\n",
        "        \"\"\"\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Progress bar for training\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1} Training\")\n",
        "\n",
        "        for batch_idx, batch in enumerate(progress_bar):\n",
        "            # Move batch to device\n",
        "            batch = {k: v.to(self.device, non_blocking=True) for k, v in batch.items()}\n",
        "\n",
        "            # Forward pass with mixed precision\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                outputs = model(**batch)\n",
        "                loss = outputs['loss'].float() / accumulate_grad_batches  # Normalize for gradient accumulation\n",
        "\n",
        "            # Backward pass\n",
        "            self.scaler.scale(loss).backward()\n",
        "\n",
        "            # Gradient accumulation: Update weights every `accumulate_grad_batches` steps\n",
        "            if (batch_idx + 1) % accumulate_grad_batches == 0 or (batch_idx + 1 == len(train_loader)):\n",
        "                # Gradient clipping\n",
        "                self.scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "                # Update weights\n",
        "                self.scaler.step(optimizer)\n",
        "                self.scaler.update()\n",
        "                optimizer.zero_grad()  # Reset gradients after updating weights\n",
        "\n",
        "            # Accumulate loss\n",
        "            total_loss += loss.item() * accumulate_grad_batches\n",
        "\n",
        "            # Log training loss to TensorBoard and WandB\n",
        "            step = epoch * len(train_loader) + batch_idx\n",
        "            self.writer.add_scalar('Train/loss', loss.item(), step)\n",
        "            wandb.log({\n",
        "                \"Train/loss\": loss.item(),\n",
        "                \"step\": step,\n",
        "                \"epoch\": epoch,\n",
        "                \"learning_rate\": optimizer.param_groups[0]['lr']\n",
        "            })\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        return avg_loss\n",
        "\n",
        "    def evaluate(self, model, data_loader):\n",
        "        \"\"\"\n",
        "        Evaluate model performance on a dataset.\n",
        "\n",
        "        Args:\n",
        "            model: The model to evaluate.\n",
        "            data_loader: DataLoader for the evaluation data.\n",
        "\n",
        "        Returns:\n",
        "            Tuple containing (loss, f1_score, roc_auc, accuracy).\n",
        "        \"\"\"\n",
        "        model.eval()\n",
        "        total_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        all_probs = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in data_loader:\n",
        "                batch = {k: v.to(self.device, non_blocking=True) for k, v in batch.items()}\n",
        "\n",
        "                with torch.amp.autocast('cuda'):\n",
        "                    outputs = model(**batch)\n",
        "                    if 'loss' in outputs:\n",
        "                        loss = outputs['loss']\n",
        "                        total_loss += loss.item()\n",
        "\n",
        "                logits = outputs['logits']\n",
        "                labels = batch['labels'].cpu().numpy()\n",
        "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "                probs = F.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
        "\n",
        "                all_preds.extend(preds)\n",
        "                all_labels.extend(labels)\n",
        "                all_probs.extend(probs)\n",
        "\n",
        "        # Compute metrics\n",
        "        unique_classes = np.unique(all_labels)\n",
        "        metrics = {\n",
        "            'loss': total_loss / len(data_loader),\n",
        "            'f1': f1_score(all_labels, all_preds, average='weighted'),\n",
        "            'accuracy': accuracy_score(all_labels, all_preds),\n",
        "            'precision': precision_score(all_labels, all_preds, average='weighted'),\n",
        "            'recall': recall_score(all_labels, all_preds, average='weighted')\n",
        "        }\n",
        "\n",
        "        # Compute ROC-AUC if binary classification\n",
        "        if len(unique_classes) == 2:\n",
        "            metrics['roc_auc'] = roc_auc_score(all_labels, all_probs)\n",
        "        else:\n",
        "            metrics['roc_auc'] = 0.5  # Default for non-binary cases\n",
        "\n",
        "        # Log metrics to TensorBoard and WandB\n",
        "        for metric_name, value in metrics.items():\n",
        "            self.writer.add_scalar(f'val/{metric_name}', value)\n",
        "            wandb.log({f\"val_{metric_name}\": value})\n",
        "\n",
        "        return metrics['loss'], metrics['f1'], metrics['roc_auc'], metrics['accuracy']\n",
        "\n",
        "    def train_and_compare(self):\n",
        "        \"\"\"\n",
        "        Train and compare both models through multiple epochs with early stopping.\n",
        "        \"\"\"\n",
        "        early_stopping_patience = 3\n",
        "        best_val_loss = float('inf')\n",
        "        best_maxpool_state = None\n",
        "        best_globalpool_state = None\n",
        "        no_improve_epochs = 0\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            print(f\"\\nEpoch {epoch + 1}/{self.num_epochs}\")\n",
        "\n",
        "            # Train MaxPool model\n",
        "            train_loss_maxpool = self.train_epoch(\n",
        "                self.model_maxpool,\n",
        "                self.optimizer['MaxPool'],\n",
        "                self.train_loader,\n",
        "                self.accumulate_grad_batches,\n",
        "                epoch\n",
        "            )\n",
        "            val_loss_maxpool, val_f1_maxpool, val_roc_auc_maxpool, val_accuracy_maxpool = self.evaluate(\n",
        "                self.model_maxpool,\n",
        "                self.val_loader\n",
        "            )\n",
        "\n",
        "            # Store results for MaxPool\n",
        "            self.results['MaxPool']['train_loss'].append(train_loss_maxpool)\n",
        "            self.results['MaxPool']['val_loss'].append(val_loss_maxpool)\n",
        "            self.results['MaxPool']['val_f1'].append(val_f1_maxpool)\n",
        "            self.results['MaxPool']['val_accuracy'].append(val_accuracy_maxpool)\n",
        "\n",
        "            # Update best weights for MaxPool\n",
        "            if val_loss_maxpool < best_val_loss:\n",
        "                best_val_loss = val_loss_maxpool\n",
        "                best_maxpool_state = self.model_maxpool.state_dict().copy()\n",
        "                no_improve_epochs = 0\n",
        "\n",
        "            self.scheduler['MaxPool'].step()\n",
        "\n",
        "            # Train GlobalPool model\n",
        "            train_loss_globalpool = self.train_epoch(\n",
        "                self.model_globalpool,\n",
        "                self.optimizer['GlobalPool'],\n",
        "                self.train_loader,\n",
        "                self.accumulate_grad_batches,\n",
        "                epoch\n",
        "            )\n",
        "            val_loss_globalpool, val_f1_globalpool, val_roc_auc_globalpool, val_accuracy_globalpool = self.evaluate(\n",
        "                self.model_globalpool,\n",
        "                self.val_loader\n",
        "            )\n",
        "\n",
        "            # Store results for GlobalPool\n",
        "            self.results['GlobalPool']['train_loss'].append(train_loss_globalpool)\n",
        "            self.results['GlobalPool']['val_loss'].append(val_loss_globalpool)\n",
        "            self.results['GlobalPool']['val_f1'].append(val_f1_globalpool)\n",
        "            self.results['GlobalPool']['val_accuracy'].append(val_accuracy_globalpool)\n",
        "\n",
        "            # Update best weights for GlobalPool\n",
        "            if val_loss_globalpool < best_val_loss:\n",
        "                best_val_loss = val_loss_globalpool\n",
        "                best_globalpool_state = copy.deepcopy(self.model_globalpool.state_dict())\n",
        "                no_improve_epochs = 0\n",
        "            else:\n",
        "                no_improve_epochs += 1\n",
        "\n",
        "            self.scheduler['GlobalPool'].step()\n",
        "\n",
        "            # Early stopping check\n",
        "            if no_improve_epochs >= early_stopping_patience:\n",
        "                print(\"🛑 Early stopping triggered. Training stopped.\")\n",
        "                break\n",
        "\n",
        "            # Log results for this epoch\n",
        "            print(f\"\\nMaxPool - Val Loss: {val_loss_maxpool:.4f}, Val F1: {val_f1_maxpool:.4f}, Val Accuracy: {val_accuracy_maxpool:.4f}\")\n",
        "            print(f\"GlobalPool - Val Loss: {val_loss_globalpool:.4f}, Val F1: {val_f1_globalpool:.4f}, Val Accuracy: {val_accuracy_globalpool:.4f}\")\n",
        "\n",
        "        # Load best weights\n",
        "        if best_maxpool_state:\n",
        "            self.model_maxpool.load_state_dict(best_maxpool_state)\n",
        "        if best_globalpool_state:\n",
        "            self.model_globalpool.load_state_dict(best_globalpool_state)\n",
        "\n",
        "    def final_evaluation(self):\n",
        "        \"\"\"\n",
        "        Perform final evaluation on the test set for both models.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing confusion matrices and classification reports for both models.\n",
        "        \"\"\"\n",
        "        results = {}\n",
        "        for name, model in [('MaxPool', self.model_maxpool), ('GlobalPool', self.model_globalpool)]:\n",
        "            model.eval()\n",
        "            all_preds = []\n",
        "            all_labels = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in self.test_loader:\n",
        "                    input_ids = batch['input_ids'].to(self.device)\n",
        "                    attention_mask = batch['attention_mask'].to(self.device)\n",
        "                    features = batch['features'].to(self.device)\n",
        "                    labels = batch['labels'].to(self.device)\n",
        "\n",
        "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, features=features)\n",
        "                    preds = torch.argmax(outputs['logits'], dim=1)\n",
        "                    all_preds.extend(preds.detach().cpu().numpy())\n",
        "                    all_labels.extend(labels.detach().cpu().numpy())\n",
        "\n",
        "            results[name] = {\n",
        "                'confusion_matrix': confusion_matrix(all_labels, all_preds),\n",
        "                'classification_report': classification_report(all_labels, all_preds, output_dict=True)\n",
        "            }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def plot_comparison(self):\n",
        "        \"\"\"\n",
        "        Plot comparison of training and validation metrics for both models.\n",
        "\n",
        "        Returns:\n",
        "            Matplotlib figure object.\n",
        "        \"\"\"\n",
        "        sns.set_theme(style=\"whitegrid\", font_scale=1.2)\n",
        "        fig = plt.figure(figsize=(20, 20))\n",
        "        gs = fig.add_gridspec(4, 2)\n",
        "\n",
        "        # Training and Validation Loss\n",
        "        ax1 = fig.add_subplot(gs[0, 0])\n",
        "        epochs = range(1, len(self.results['MaxPool']['train_loss']) + 1)\n",
        "        ax1.plot(epochs, self.results['MaxPool']['train_loss'], 'b--', label='MaxPool (Train)')\n",
        "        ax1.plot(epochs, self.results['MaxPool']['val_loss'], 'b-', label='MaxPool (Val)')\n",
        "        ax1.plot(epochs, self.results['GlobalPool']['train_loss'], 'r--', label='GlobalPool (Train)')\n",
        "        ax1.plot(epochs, self.results['GlobalPool']['val_loss'], 'r-', label='GlobalPool (Val)')\n",
        "        ax1.set_title('Training and Validation Loss')\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.set_ylabel('Loss')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True)\n",
        "\n",
        "        # Validation F1 Score\n",
        "        ax2 = fig.add_subplot(gs[0, 1])\n",
        "        ax2.plot(epochs, self.results['MaxPool']['val_f1'], 'b-', label='MaxPool')\n",
        "        ax2.plot(epochs, self.results['GlobalPool']['val_f1'], 'r-', label='GlobalPool')\n",
        "        ax2.set_title('Validation F1 Score')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.set_ylabel('F1 Score')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True)\n",
        "\n",
        "        # Validation Accuracy\n",
        "        ax3 = fig.add_subplot(gs[1, 0])\n",
        "        ax3.plot(epochs, self.results['MaxPool']['val_accuracy'], 'b-', label='MaxPool')\n",
        "        ax3.plot(epochs, self.results['GlobalPool']['val_accuracy'], 'r-', label='GlobalPool')\n",
        "        ax3.set_title('Validation Accuracy')\n",
        "        ax3.set_xlabel('Epoch')\n",
        "        ax3.set_ylabel('Accuracy')\n",
        "        ax3.legend()\n",
        "        ax3.grid(True)\n",
        "\n",
        "        # Confusion Matrix for MaxPool\n",
        "        ax4 = fig.add_subplot(gs[1, 1])\n",
        "        final_results = self.final_evaluation()\n",
        "        sns.heatmap(final_results['MaxPool']['confusion_matrix'], annot=True, fmt='d', cmap='Blues', ax=ax4)\n",
        "        ax4.set_title('Confusion Matrix - MaxPool')\n",
        "        ax4.set_xlabel('Predicted')\n",
        "        ax4.set_ylabel('True')\n",
        "\n",
        "        # Confusion Matrix for GlobalPool\n",
        "        ax5 = fig.add_subplot(gs[2, 0])\n",
        "        sns.heatmap(final_results['GlobalPool']['confusion_matrix'], annot=True, fmt='d', cmap='Reds', ax=ax5)\n",
        "        ax5.set_title('Confusion Matrix - GlobalPool')\n",
        "        ax5.set_xlabel('Predicted')\n",
        "        ax5.set_ylabel('True')\n",
        "\n",
        "        # Performance Metrics Comparison\n",
        "        ax6 = fig.add_subplot(gs[2, 1])\n",
        "        metrics = ['precision', 'recall', 'f1-score', 'accuracy']\n",
        "        maxpool_scores = [final_results['MaxPool']['classification_report']['weighted avg'][m] for m in metrics[:-1]] + [final_results['MaxPool']['classification_report']['accuracy']]\n",
        "        globalpool_scores = [final_results['GlobalPool']['classification_report']['weighted avg'][m] for m in metrics[:-1]] + [final_results['GlobalPool']['classification_report']['accuracy']]\n",
        "        x = np.arange(len(metrics))\n",
        "        width = 0.35\n",
        "        ax6.bar(x - width/2, maxpool_scores, width, label='MaxPool', color='blue', alpha=0.6)\n",
        "        ax6.bar(x + width/2, globalpool_scores, width, label='GlobalPool', color='red', alpha=0.6)\n",
        "        ax6.set_title('Performance Metrics Comparison')\n",
        "        ax6.set_xticks(x)\n",
        "        ax6.set_xticklabels(metrics)\n",
        "        ax6.legend()\n",
        "\n",
        "        # Smoothed Training Loss Progress\n",
        "        ax7 = fig.add_subplot(gs[3, 0])\n",
        "        window_size = 5\n",
        "        maxpool_smooth = np.convolve(self.results['MaxPool']['train_loss'], np.ones(window_size)/window_size, mode='valid')\n",
        "        globalpool_smooth = np.convolve(self.results['GlobalPool']['train_loss'], np.ones(window_size)/window_size, mode='valid')\n",
        "        ax7.plot(maxpool_smooth, label='MaxPool')\n",
        "        ax7.plot(globalpool_smooth, label='GlobalPool')\n",
        "        ax7.set_title('Smoothed Training Loss Progress')\n",
        "        ax7.set_xlabel('Training Steps')\n",
        "        ax7.set_ylabel('Smoothed Loss')\n",
        "        ax7.legend()\n",
        "        ax7.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig"
      ],
      "metadata": {
        "id": "7bIGqFUsDLoF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# تابع convert_to_serializable را خارج از کلاس تعریف کنید\n",
        "def convert_to_serializable(obj):\n",
        "    \"\"\"\n",
        "    تبدیل تنسورها و آرایه‌های نامطلوب به مقادیر قابل سریالیزه\n",
        "    \"\"\"\n",
        "    if isinstance(obj, torch.Tensor):\n",
        "        return obj.cpu().detach().numpy().tolist()\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    elif isinstance(obj, dict):\n",
        "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [convert_to_serializable(v) for v in obj]\n",
        "    elif isinstance(obj, tuple):\n",
        "        return tuple(convert_to_serializable(v) for v in obj)\n",
        "    elif hasattr(obj, 'item') and callable(getattr(obj, 'item')):\n",
        "        return obj.item()\n",
        "    elif isinstance(obj, (np.int32, np.int64, np.float32, np.float64)):\n",
        "        return obj.item()\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "class StratifiedKFoldTrainer:\n",
        "    def __init__(self, n_splits=5, random_state=42):\n",
        "        self.n_splits = n_splits\n",
        "        self.random_state = random_state\n",
        "        self.device = torch.device('cuda')\n",
        "        self.writer = SummaryWriter('runs/anxiety_detection_kfold')\n",
        "        self.scaler = torch.amp.GradScaler()\n",
        "\n",
        "    def create_fold_datasets(self, df, tokenizer, fold_idx, train_idx, val_idx):\n",
        "        \"\"\"Create training and validation datasets for a specific fold\"\"\"\n",
        "        train_data = df.iloc[train_idx]\n",
        "        val_data = df.iloc[val_idx]\n",
        "\n",
        "        train_dataset = AnxietyDataset(\n",
        "            train_data['text'].values,\n",
        "            train_data['label'].values,\n",
        "            tokenizer\n",
        "        )\n",
        "        val_dataset = AnxietyDataset(\n",
        "            val_data['text'].values,\n",
        "            val_data['label'].values,\n",
        "            tokenizer\n",
        "        )\n",
        "\n",
        "        return train_dataset, val_dataset\n",
        "\n",
        "    def train_fold(self, model, train_dataset, val_dataset, fold_idx):\n",
        "        \"\"\"Train model on a specific fold\"\"\"\n",
        "        train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, pin_memory=True, persistent_workers=True, num_workers=2, drop_last=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=8)\n",
        "\n",
        "        # Calculate class weights for this fold\n",
        "        all_labels = []\n",
        "        for batch in train_loader:\n",
        "            all_labels.extend(batch['labels'].cpu().numpy().tolist())\n",
        "\n",
        "        class_weights = compute_class_weight(\n",
        "            class_weight=\"balanced\",\n",
        "            classes=np.unique(all_labels),\n",
        "            y=all_labels\n",
        "        )\n",
        "        class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.device)\n",
        "\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='min', factor=0.5, patience=2\n",
        "        )\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        early_stopping_counter = 0\n",
        "        early_stopping_patience = 3\n",
        "        fold_results = {\n",
        "            'train_loss': [], 'val_loss': [], 'val_f1': [],\n",
        "            'best_model_state': None, 'best_val_metrics': None\n",
        "        }\n",
        "\n",
        "        for epoch in range(2):  # 5 epochs per fold\n",
        "            # Training\n",
        "            model.train()\n",
        "            total_train_loss = 0\n",
        "\n",
        "            for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Fold {fold_idx+1} Epoch {epoch+1}\")):\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                batch = {k: v.to(self.device, non_blocking=True) for k, v in batch.items()}\n",
        "\n",
        "                # Forward pass with mixed precision\n",
        "                with torch.amp.autocast(device_type='cuda'):\n",
        "                    outputs = model(**batch)\n",
        "                    loss = outputs['loss']\n",
        "\n",
        "                # Backward pass with gradient scaling\n",
        "                self.scaler.scale(loss).backward()\n",
        "                self.scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                self.scaler.step(optimizer)\n",
        "                self.scaler.update()\n",
        "\n",
        "                total_train_loss += loss.item()\n",
        "\n",
        "                # Log metrics\n",
        "                self.writer.add_scalar(\n",
        "                    f'Fold_{fold_idx+1}/train_loss',\n",
        "                    loss.item(),\n",
        "                    epoch * len(train_loader) + batch_idx\n",
        "                )\n",
        "\n",
        "            avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "            # Validation\n",
        "            model.eval()\n",
        "            val_metrics = self.evaluate_model(model, val_loader)\n",
        "\n",
        "            # Update learning rate\n",
        "            scheduler.step(val_metrics['loss'])\n",
        "\n",
        "            # Early stopping check\n",
        "            if val_metrics['loss'] < best_val_loss:\n",
        "                best_val_loss = val_metrics['loss']\n",
        "                fold_results['best_model_state'] = copy.deepcopy(model.state_dict())\n",
        "                fold_results['best_val_metrics'] = val_metrics\n",
        "                early_stopping_counter = 0\n",
        "            else:\n",
        "                early_stopping_counter += 1\n",
        "\n",
        "            if early_stopping_counter >= early_stopping_patience:\n",
        "                print(f\"Early stopping triggered in fold {fold_idx+1}\")\n",
        "                break\n",
        "\n",
        "            # Store results\n",
        "            fold_results['train_loss'].append(avg_train_loss)\n",
        "            fold_results['val_loss'].append(val_metrics['loss'])\n",
        "            fold_results['val_f1'].append(val_metrics['f1'])\n",
        "\n",
        "            # Log fold metrics\n",
        "            for metric_name, value in val_metrics.items():\n",
        "                self.writer.add_scalar(\n",
        "                    f'Fold_{fold_idx+1}/val_{metric_name}',\n",
        "                    value,\n",
        "                    epoch\n",
        "                )\n",
        "\n",
        "        return fold_results\n",
        "\n",
        "    def evaluate_model(self, model, data_loader):\n",
        "        \"\"\"Evaluate model performance\"\"\"\n",
        "        model.eval()\n",
        "        total_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        all_probs = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in data_loader:\n",
        "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
        "                outputs = model(**batch)\n",
        "\n",
        "                loss = outputs['loss']\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                logits = outputs['logits']\n",
        "                probs = F.softmax(logits, dim=1)\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(batch['labels'].cpu().numpy())\n",
        "                all_probs.extend(probs[:, 1].cpu().numpy())\n",
        "\n",
        "        # بررسی تعداد کلاس‌های موجود در `all_labels`\n",
        "        unique_classes = np.unique(all_labels)\n",
        "\n",
        "        if len(unique_classes) == 2:\n",
        "            # برای دو کلاس، `multi_class` نیاز نیست\n",
        "            roc_auc = roc_auc_score(all_labels, all_probs)\n",
        "        else:\n",
        "            # برای چندین کلاس، `multi_class='ovr'`\n",
        "            roc_auc = roc_auc_score(all_labels, all_probs, multi_class='ovr')\n",
        "\n",
        "        metrics = {\n",
        "            'loss': total_loss / len(data_loader),\n",
        "            'f1': f1_score(all_labels, all_preds, average='weighted'),\n",
        "            'accuracy': accuracy_score(all_labels, all_preds),\n",
        "            'precision': precision_score(all_labels, all_preds, average='weighted'),\n",
        "            'recall': recall_score(all_labels, all_preds, average='weighted'),\n",
        "            'roc_auc': roc_auc\n",
        "        }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def calculate_average_metrics(self, fold_results):\n",
        "        \"\"\"Calculate average metrics across all folds\"\"\"\n",
        "        all_metrics = defaultdict(list)\n",
        "\n",
        "        for fold_result in fold_results:\n",
        "            for metric, value in fold_result['best_val_metrics'].items():\n",
        "                # تبدیل تنسورها به اعداد float\n",
        "                if isinstance(value, torch.Tensor):\n",
        "                    value = value.detach().cpu().numpy().item()\n",
        "                elif isinstance(value, np.ndarray):\n",
        "                    value = value.item() if value.size == 1 else value.tolist()\n",
        "                elif isinstance(value, (np.float32, np.float64, np.int32, np.int64)):\n",
        "                    value = value.item()\n",
        "\n",
        "                all_metrics[metric].append(value)\n",
        "\n",
        "        avg_metrics = {\n",
        "            metric: {\n",
        "                'mean': float(np.mean(values)),\n",
        "                'std': float(np.std(values))\n",
        "            }\n",
        "            for metric, values in all_metrics.items()\n",
        "        }\n",
        "\n",
        "        return avg_metrics\n",
        "\n",
        "    def log_final_results(self, avg_metrics):\n",
        "        \"\"\"Log final results and create visualizations\"\"\"\n",
        "        print(\"\\nFinal Cross-Validation Results:\")\n",
        "        for metric, stats in avg_metrics.items():\n",
        "            print(f\"{metric}:\")\n",
        "            print(f\"  Mean: {stats['mean']:.4f}\")\n",
        "            print(f\"  Std:  {stats['std']:.4f}\")\n",
        "\n",
        "            # تبدیل به مقادیر عددی قبل از لاگ\n",
        "            mean_val = float(stats['mean'])\n",
        "            std_val = float(stats['std'])\n",
        "\n",
        "            self.writer.add_scalar(f'Final/avg_{metric}', mean_val)\n",
        "            self.writer.add_scalar(f'Final/std_{metric}', std_val)\n",
        "\n",
        "    def train_with_kfold(self, df, tokenizer, model_class):\n",
        "        \"\"\"Perform k-fold cross validation training\"\"\"\n",
        "        skf = StratifiedKFold(\n",
        "            n_splits=self.n_splits,\n",
        "            shuffle=True,\n",
        "            random_state=self.random_state\n",
        "        )\n",
        "\n",
        "        fold_results = []\n",
        "        overall_best_model = None\n",
        "        best_val_f1 = 0\n",
        "\n",
        "        for fold_idx, (train_idx, val_idx) in enumerate(skf.split(df, df['label'])):\n",
        "            print(f\"\\nTraining Fold {fold_idx + 1}/{self.n_splits}\")\n",
        "\n",
        "            # Create datasets for this fold\n",
        "            train_dataset, val_dataset = self.create_fold_datasets(\n",
        "                df, tokenizer, fold_idx, train_idx, val_idx\n",
        "            )\n",
        "\n",
        "            # Initialize a new model for this fold\n",
        "            config = AutoConfig.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
        "            if model_class == GlobalPoolCNNModel:\n",
        "                model = model_class(config, train_labels=df['label'].values).to(self.device)\n",
        "            else:\n",
        "                model = model_class(config).to(self.device)\n",
        "\n",
        "            # Train the model on this fold\n",
        "            fold_result = self.train_fold(model, train_dataset, val_dataset, fold_idx)\n",
        "\n",
        "            # تبدیل نتایج fold به فرمت قابل سریالیزه قبل از اضافه کردن به لیست\n",
        "            serializable_fold_result = {\n",
        "                'train_loss': [float(loss) for loss in fold_result['train_loss']],\n",
        "                'val_loss': [float(loss) for loss in fold_result['val_loss']],\n",
        "                'val_f1': [float(f1) for f1 in fold_result['val_f1']],\n",
        "                'best_val_metrics': convert_to_serializable(fold_result['best_val_metrics'])\n",
        "                # best_model_state را نگه نمیداریم چون نیازی به سریالیزه کردن آن نیست\n",
        "            }\n",
        "\n",
        "            fold_results.append(serializable_fold_result)\n",
        "\n",
        "            # Update best model if needed\n",
        "            if fold_result['best_val_metrics']['f1'] > best_val_f1:\n",
        "                best_val_f1 = fold_result['best_val_metrics']['f1']\n",
        "                overall_best_model = copy.deepcopy(fold_result['best_model_state'])\n",
        "\n",
        "        # Calculate and log average metrics across folds\n",
        "        avg_metrics = self.calculate_average_metrics(fold_results)\n",
        "        self.log_final_results(avg_metrics)\n",
        "\n",
        "        # تبدیل نتایج نهایی به فرمت قابل سریالیزه\n",
        "        kfold_results = {\n",
        "            'metrics': convert_to_serializable(avg_metrics),\n",
        "            'fold_results': fold_results\n",
        "        }\n",
        "\n",
        "        os.makedirs(\"experiment_results/kfold\", exist_ok=True)\n",
        "        with open(os.path.join(\"experiment_results/kfold\", f'cross_validation_results_{model_class.__name__}.json'), 'w') as f:\n",
        "            json.dump(kfold_results, f, indent=2)\n",
        "\n",
        "        return overall_best_model, avg_metrics, fold_results\n",
        "\n"
      ],
      "metadata": {
        "id": "7mA2UUeDDN4J"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yUt9JGWKDcgG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PE1r5wonDmnA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import DistilBertModel\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class MaxPoolCNNModel(nn.Module):\n",
        "    def __init__(self, config, max_length=128, train_labels=None):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.max_length = max_length\n",
        "        self.embedding_dim = config.hidden_size\n",
        "\n",
        "        # Load pre-trained DistilBERT\n",
        "        self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.bert.gradient_checkpointing_enable()\n",
        "\n",
        "        # CNN layers\n",
        "        self.conv1 = nn.Conv1d(self.embedding_dim, 256, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(256, 128, kernel_size=3, padding=1)\n",
        "        nn.init.kaiming_normal_(self.conv1.weight, nonlinearity='relu')\n",
        "        nn.init.kaiming_normal_(self.conv2.weight, nonlinearity='relu')\n",
        "\n",
        "        # Pooling and normalization\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.batch_norm1 = nn.BatchNorm1d(256)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(128)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "        # Classifier\n",
        "        pooled_output_size = 128 * (max_length // 4)  # After 2x pooling\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(pooled_output_size, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, 2)  # You can make `num_classes` configurable\n",
        "        )\n",
        "        for layer in self.classifier:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                nn.init.xavier_normal_(layer.weight)\n",
        "\n",
        "        # Class weights\n",
        "        self.class_weights = None\n",
        "        if train_labels is not None:\n",
        "            self.compute_class_weights(train_labels)\n",
        "\n",
        "        self.to(self.device)\n",
        "\n",
        "    def compute_class_weights(self, train_labels):\n",
        "        try:\n",
        "            if isinstance(train_labels, (np.ndarray, torch.Tensor)):\n",
        "                train_labels = train_labels.tolist()\n",
        "            train_labels = [label for label in train_labels if label is not None]\n",
        "\n",
        "            unique_classes = np.unique(train_labels)\n",
        "            if len(unique_classes) > 1:\n",
        "                class_weights = compute_class_weight(\n",
        "                    class_weight=\"balanced\",\n",
        "                    classes=unique_classes,\n",
        "                    y=train_labels\n",
        "                )\n",
        "                self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.device)\n",
        "                logger.info(f\"Class weights: {self.class_weights}\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error computing class weights: {e}\")\n",
        "            self.class_weights = None\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        input_ids = input_ids.to(self.device)\n",
        "        attention_mask = attention_mask.to(self.device)\n",
        "        if labels is not None:\n",
        "            labels = labels.to(self.device).long()  # اطمینان از نوع torch.long برای labels\n",
        "\n",
        "        # Mixed precision block\n",
        "        with torch.amp.autocast(device_type=self.device.type):\n",
        "            # BERT encoding\n",
        "            bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            sequence_output = bert_output.last_hidden_state  # shape: (batch, seq_len, hidden)\n",
        "\n",
        "            # CNN forward\n",
        "            x = sequence_output.transpose(1, 2)  # (batch, hidden, seq_len)\n",
        "            x = self.pool(F.relu(self.batch_norm1(self.conv1(x))))\n",
        "            x = self.pool(F.relu(self.batch_norm2(self.conv2(x))))\n",
        "            x = x.view(x.size(0), -1)  # flatten\n",
        "            x = self.dropout(x)\n",
        "            logits = self.classifier(x)\n",
        "\n",
        "        result = {'logits': logits}\n",
        "\n",
        "        if labels is not None:\n",
        "            # Use class_weights only if available, and keep in float32\n",
        "            if self.class_weights is not None:\n",
        "                weight = self.class_weights.to(dtype=torch.float32)  # اطمینان از FP32\n",
        "                loss_fct = nn.CrossEntropyLoss(weight=weight)\n",
        "            else:\n",
        "                loss_fct = nn.CrossEntropyLoss()\n",
        "\n",
        "            # Convert logits to float32 explicitly\n",
        "            loss = loss_fct(logits.float(), labels)\n",
        "            result['loss'] = loss\n",
        "\n",
        "        return result"
      ],
      "metadata": {
        "id": "TLJscbFlFmeG"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import DistilBertModel\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class GlobalPoolCNNModel(nn.Module):\n",
        "    def __init__(self, config, max_length=128, train_labels=None):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.embedding_dim = config.hidden_size\n",
        "\n",
        "        # Load pre-trained DistilBERT\n",
        "        self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.bert.gradient_checkpointing_enable()\n",
        "\n",
        "        # CNN layers\n",
        "        self.conv1 = nn.Conv1d(self.embedding_dim, 128, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(128, 64, kernel_size=3, padding=1)\n",
        "\n",
        "        # Residual connection\n",
        "        self.use_residual = True\n",
        "        self.projection = nn.Conv1d(self.embedding_dim, 128, kernel_size=1) if self.use_residual else None\n",
        "\n",
        "        # Weight initialization\n",
        "        nn.init.kaiming_normal_(self.conv1.weight, nonlinearity='relu')\n",
        "        nn.init.kaiming_normal_(self.conv2.weight, nonlinearity='relu')\n",
        "        if self.projection:\n",
        "            nn.init.kaiming_normal_(self.projection.weight, nonlinearity='relu')\n",
        "\n",
        "        # Pooling layers\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.global_max_pool = nn.AdaptiveMaxPool1d(1)\n",
        "\n",
        "        # Normalization and dropout\n",
        "        self.batch_norm1 = nn.BatchNorm1d(128)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(64)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "        # Classifier: input dim = 64 (avg) + 64 (max) = 128\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(128, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, 2)  # Binary classification\n",
        "        )\n",
        "        for layer in self.classifier:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                nn.init.xavier_normal_(layer.weight)\n",
        "\n",
        "        # Compute class weights if labels are provided\n",
        "        self.class_weights = None\n",
        "        if train_labels is not None:\n",
        "            self.compute_class_weights(train_labels)\n",
        "\n",
        "        self.to(self.device)\n",
        "\n",
        "    def compute_class_weights(self, train_labels):\n",
        "        try:\n",
        "            if isinstance(train_labels, (np.ndarray, torch.Tensor)):\n",
        "                train_labels = train_labels.tolist()\n",
        "            train_labels = [label for label in train_labels if label is not None]\n",
        "\n",
        "            unique_classes = np.unique(train_labels)\n",
        "            if len(unique_classes) > 1:\n",
        "                weights = compute_class_weight(\n",
        "                    class_weight='balanced',\n",
        "                    classes=unique_classes,\n",
        "                    y=train_labels\n",
        "                )\n",
        "                self.class_weights = torch.tensor(weights, dtype=torch.float32).to(self.device)\n",
        "                logger.info(f\"Class weights computed: {self.class_weights}\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to compute class weights: {e}\")\n",
        "            self.class_weights = None\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        input_ids = input_ids.to(self.device)\n",
        "        attention_mask = attention_mask.to(self.device)\n",
        "        if labels is not None:\n",
        "            labels = labels.to(self.device).long()  # اطمینان از نوع torch.long برای labels\n",
        "\n",
        "        with torch.amp.autocast(device_type=self.device.type):\n",
        "            # BERT output\n",
        "            bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            sequence_output = bert_output.last_hidden_state  # (batch, seq_len, hidden)\n",
        "\n",
        "            x = sequence_output.transpose(1, 2)  # (batch, hidden, seq_len)\n",
        "\n",
        "            # First conv + optional residual\n",
        "            out1 = self.conv1(x)\n",
        "            out1 = self.batch_norm1(out1)\n",
        "            if self.use_residual and self.projection is not None:\n",
        "                residual = self.projection(x)\n",
        "                out1 = out1 + residual\n",
        "            out1 = F.relu(out1)\n",
        "\n",
        "            # Second conv\n",
        "            out2 = self.conv2(out1)\n",
        "            out2 = self.batch_norm2(out2)\n",
        "            out2 = F.relu(out2)\n",
        "\n",
        "            # Global pooling\n",
        "            avg_pooled = self.global_avg_pool(out2).squeeze(-1)  # (batch, 64)\n",
        "            max_pooled = self.global_max_pool(out2).squeeze(-1)  # (batch, 64)\n",
        "            x = torch.cat([avg_pooled, max_pooled], dim=1)  # (batch, 128)\n",
        "\n",
        "            x = self.dropout(x)\n",
        "            logits = self.classifier(x)\n",
        "\n",
        "        result = {\"logits\": logits}\n",
        "\n",
        "        if labels is not None:\n",
        "            # Use class_weights only if available, and keep in float32\n",
        "            if self.class_weights is not None:\n",
        "                weight = self.class_weights.to(dtype=torch.float32)  # اطمینان از FP32\n",
        "                loss_fct = nn.CrossEntropyLoss(weight=weight)\n",
        "            else:\n",
        "                loss_fct = nn.CrossEntropyLoss()\n",
        "\n",
        "            # Convert logits to float32 explicitly\n",
        "            loss = loss_fct(logits.float(), labels)\n",
        "            result[\"loss\"] = loss\n",
        "\n",
        "        return result"
      ],
      "metadata": {
        "id": "qKq2LMBgFq4u"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import wandb\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import (\n",
        "    f1_score, accuracy_score, confusion_matrix, classification_report,\n",
        "    precision_score, recall_score, roc_auc_score\n",
        ")\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "\n",
        "class ModelComparison:\n",
        "    def __init__(self, model_maxpool, model_globalpool, train_dataset, val_dataset, test_dataset, batch_size=16):\n",
        "        \"\"\"\n",
        "        Initialize the ModelComparison class for training and comparing two models.\n",
        "\n",
        "        Args:\n",
        "            model_maxpool: MaxPoolCNNModel instance.\n",
        "            model_globalpool: GlobalPoolCNNModel instance.\n",
        "            train_dataset: Training dataset.\n",
        "            val_dataset: Validation dataset.\n",
        "            test_dataset: Test dataset.\n",
        "            batch_size: Batch size for data loaders.\n",
        "        \"\"\"\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Set up models\n",
        "        self.model_maxpool = model_maxpool.to(self.device)\n",
        "        self.model_globalpool = model_globalpool.to(self.device)\n",
        "\n",
        "        # Enable PyTorch 2.0+ optimizations if available\n",
        "        if hasattr(torch, 'compile'):\n",
        "            self.model_maxpool = torch.compile(self.model_maxpool)\n",
        "            self.model_globalpool = torch.compile(self.model_globalpool)\n",
        "\n",
        "        # Mixed precision training\n",
        "        self.scaler = torch.amp.GradScaler()\n",
        "\n",
        "        # Batch size and gradient accumulation\n",
        "        self.batch_size = batch_size\n",
        "        self.accumulate_grad_batches = 4\n",
        "\n",
        "        # Data loaders with optimized settings\n",
        "        self.train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=2,\n",
        "            persistent_workers=True,\n",
        "            pin_memory=True,\n",
        "            drop_last=True\n",
        "        )\n",
        "        self.val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=4,\n",
        "            persistent_workers=True,\n",
        "            pin_memory=True\n",
        "        )\n",
        "        self.test_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=4,\n",
        "            persistent_workers=True,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        # Optimizers\n",
        "        self.optimizer = {\n",
        "            'MaxPool': torch.optim.AdamW(self.model_maxpool.parameters(), lr=3e-5, weight_decay=1e-4),\n",
        "            'GlobalPool': torch.optim.AdamW(self.model_globalpool.parameters(), lr=3e-5, weight_decay=1e-4)\n",
        "        }\n",
        "\n",
        "        # Learning rate schedulers with cosine annealing and warm restarts\n",
        "        total_steps = len(self.train_loader) * 5  # 10 epochs\n",
        "        warmup_steps = int(0.1 * total_steps)\n",
        "        self.scheduler = {\n",
        "            'MaxPool': torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "                self.optimizer['MaxPool'], T_0=10, T_mult=2, eta_min=1e-6\n",
        "            ),\n",
        "            'GlobalPool': torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "                self.optimizer['GlobalPool'], T_0=10, T_mult=2, eta_min=1e-6\n",
        "            )\n",
        "        }\n",
        "\n",
        "        # Logging setup\n",
        "        self.writer = SummaryWriter('runs/anxiety_detection_experiment')\n",
        "        wandb.init(project=\"anxiety-detection\", config={\n",
        "            \"batch_size\": batch_size,\n",
        "            \"learning_rate\": 3e-5,\n",
        "            \"architecture\": \"MaxPool_vs_GlobalPool\"\n",
        "        })\n",
        "\n",
        "        # Compute class weights for the entire dataset\n",
        "        all_labels = []\n",
        "        for batch in self.train_loader:\n",
        "            all_labels.extend(batch['labels'].cpu().numpy())\n",
        "        self.class_weights = compute_class_weight(\n",
        "            class_weight=\"balanced\",\n",
        "            classes=np.unique(all_labels),\n",
        "            y=all_labels\n",
        "        )\n",
        "        self.class_weights = torch.tensor(self.class_weights, dtype=torch.float32).to(self.device)\n",
        "\n",
        "        # Loss function with class weights\n",
        "        self.num_epochs = 5  # Increased to 10 epochs\n",
        "        self.criterion = nn.CrossEntropyLoss(weight=self.class_weights)\n",
        "\n",
        "        # Initialize results dictionary\n",
        "        self.results = {\n",
        "            'MaxPool': {'train_loss': [], 'val_loss': [], 'val_f1': [], 'val_accuracy': []},\n",
        "            'GlobalPool': {'train_loss': [], 'val_loss': [], 'val_f1': [], 'val_accuracy': []}\n",
        "        }\n",
        "\n",
        "    def train_epoch(self, model, optimizer, train_loader, accumulate_grad_batches, epoch):\n",
        "        \"\"\"\n",
        "        Train the model for one epoch.\n",
        "\n",
        "        Args:\n",
        "            model: The model to train.\n",
        "            optimizer: The optimizer to use.\n",
        "            train_loader: DataLoader for the training data.\n",
        "            accumulate_grad_batches: Number of batches to accumulate gradients before updating weights.\n",
        "            epoch: Current epoch number.\n",
        "\n",
        "        Returns:\n",
        "            Average training loss for the epoch.\n",
        "        \"\"\"\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Progress bar for training\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1} Training\")\n",
        "\n",
        "        for batch_idx, batch in enumerate(progress_bar):\n",
        "            # Move batch to device\n",
        "            batch = {k: v.to(self.device, non_blocking=True) for k, v in batch.items()}\n",
        "\n",
        "            # Forward pass with mixed precision\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                outputs = model(**batch)\n",
        "                loss = outputs['loss'].float() / accumulate_grad_batches  # Normalize for gradient accumulation\n",
        "\n",
        "            # Backward pass\n",
        "            self.scaler.scale(loss).backward()\n",
        "\n",
        "            # Gradient accumulation: Update weights every `accumulate_grad_batches` steps\n",
        "            if (batch_idx + 1) % accumulate_grad_batches == 0 or (batch_idx + 1 == len(train_loader)):\n",
        "                # Gradient clipping\n",
        "                self.scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "                # Update weights\n",
        "                self.scaler.step(optimizer)\n",
        "                self.scaler.update()\n",
        "                optimizer.zero_grad()  # Reset gradients after updating weights\n",
        "\n",
        "            # Accumulate loss\n",
        "            total_loss += loss.item() * accumulate_grad_batches\n",
        "\n",
        "            # Log training loss to TensorBoard and WandB\n",
        "            step = epoch * len(train_loader) + batch_idx\n",
        "            self.writer.add_scalar('Train/loss', loss.item(), step)\n",
        "            wandb.log({\n",
        "                \"Train/loss\": loss.item(),\n",
        "                \"step\": step,\n",
        "                \"epoch\": epoch,\n",
        "                \"learning_rate\": optimizer.param_groups[0]['lr']\n",
        "            })\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        return avg_loss\n",
        "\n",
        "    def evaluate(self, model, data_loader):\n",
        "        \"\"\"\n",
        "        Evaluate model performance on a dataset.\n",
        "\n",
        "        Args:\n",
        "            model: The model to evaluate.\n",
        "            data_loader: DataLoader for the evaluation data.\n",
        "\n",
        "        Returns:\n",
        "            Tuple containing (loss, f1_score, roc_auc, accuracy).\n",
        "        \"\"\"\n",
        "        model.eval()\n",
        "        total_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        all_probs = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in data_loader:\n",
        "                batch = {k: v.to(self.device, non_blocking=True) for k, v in batch.items()}\n",
        "\n",
        "                with torch.amp.autocast('cuda'):\n",
        "                    outputs = model(**batch)\n",
        "                    if 'loss' in outputs:\n",
        "                        loss = outputs['loss']\n",
        "                        total_loss += loss.item()\n",
        "\n",
        "                logits = outputs['logits']\n",
        "                labels = batch['labels'].cpu().numpy()\n",
        "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "                probs = F.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
        "\n",
        "                all_preds.extend(preds)\n",
        "                all_labels.extend(labels)\n",
        "                all_probs.extend(probs)\n",
        "\n",
        "        # Compute metrics\n",
        "        unique_classes = np.unique(all_labels)\n",
        "        metrics = {\n",
        "            'loss': total_loss / len(data_loader),\n",
        "            'f1': f1_score(all_labels, all_preds, average='weighted'),\n",
        "            'accuracy': accuracy_score(all_labels, all_preds),\n",
        "            'precision': precision_score(all_labels, all_preds, average='weighted'),\n",
        "            'recall': recall_score(all_labels, all_preds, average='weighted')\n",
        "        }\n",
        "\n",
        "        # Compute ROC-AUC if binary classification\n",
        "        if len(unique_classes) == 2:\n",
        "            metrics['roc_auc'] = roc_auc_score(all_labels, all_probs)\n",
        "        else:\n",
        "            metrics['roc_auc'] = 0.5  # Default for non-binary cases\n",
        "\n",
        "        # Log metrics to TensorBoard and WandB\n",
        "        for metric_name, value in metrics.items():\n",
        "            self.writer.add_scalar(f'val/{metric_name}', value)\n",
        "            wandb.log({f\"val_{metric_name}\": value})\n",
        "\n",
        "        return metrics['loss'], metrics['f1'], metrics['roc_auc'], metrics['accuracy']\n",
        "\n",
        "    def train_and_compare(self):\n",
        "        \"\"\"\n",
        "        Train and compare both models through multiple epochs with early stopping.\n",
        "        \"\"\"\n",
        "        early_stopping_patience = 3\n",
        "        best_val_loss = float('inf')\n",
        "        best_maxpool_state = None\n",
        "        best_globalpool_state = None\n",
        "        no_improve_epochs = 0\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            print(f\"\\nEpoch {epoch + 1}/{self.num_epochs}\")\n",
        "\n",
        "            # Train MaxPool model\n",
        "            train_loss_maxpool = self.train_epoch(\n",
        "                self.model_maxpool,\n",
        "                self.optimizer['MaxPool'],\n",
        "                self.train_loader,\n",
        "                self.accumulate_grad_batches,\n",
        "                epoch\n",
        "            )\n",
        "            val_loss_maxpool, val_f1_maxpool, val_roc_auc_maxpool, val_accuracy_maxpool = self.evaluate(\n",
        "                self.model_maxpool,\n",
        "                self.val_loader\n",
        "            )\n",
        "\n",
        "            # Store results for MaxPool\n",
        "            self.results['MaxPool']['train_loss'].append(train_loss_maxpool)\n",
        "            self.results['MaxPool']['val_loss'].append(val_loss_maxpool)\n",
        "            self.results['MaxPool']['val_f1'].append(val_f1_maxpool)\n",
        "            self.results['MaxPool']['val_accuracy'].append(val_accuracy_maxpool)\n",
        "\n",
        "            # Update best weights for MaxPool\n",
        "            if val_loss_maxpool < best_val_loss:\n",
        "                best_val_loss = val_loss_maxpool\n",
        "                best_maxpool_state = self.model_maxpool.state_dict().copy()\n",
        "                no_improve_epochs = 0\n",
        "\n",
        "            self.scheduler['MaxPool'].step()\n",
        "\n",
        "            # Train GlobalPool model\n",
        "            train_loss_globalpool = self.train_epoch(\n",
        "                self.model_globalpool,\n",
        "                self.optimizer['GlobalPool'],\n",
        "                self.train_loader,\n",
        "                self.accumulate_grad_batches,\n",
        "                epoch\n",
        "            )\n",
        "            val_loss_globalpool, val_f1_globalpool, val_roc_auc_globalpool, val_accuracy_globalpool = self.evaluate(\n",
        "                self.model_globalpool,\n",
        "                self.val_loader\n",
        "            )\n",
        "\n",
        "            # Store results for GlobalPool\n",
        "            self.results['GlobalPool']['train_loss'].append(train_loss_globalpool)\n",
        "            self.results['GlobalPool']['val_loss'].append(val_loss_globalpool)\n",
        "            self.results['GlobalPool']['val_f1'].append(val_f1_globalpool)\n",
        "            self.results['GlobalPool']['val_accuracy'].append(val_accuracy_globalpool)\n",
        "\n",
        "            # Update best weights for GlobalPool\n",
        "            if val_loss_globalpool < best_val_loss:\n",
        "                best_val_loss = val_loss_globalpool\n",
        "                best_globalpool_state = copy.deepcopy(self.model_globalpool.state_dict())\n",
        "                no_improve_epochs = 0\n",
        "            else:\n",
        "                no_improve_epochs += 1\n",
        "\n",
        "            self.scheduler['GlobalPool'].step()\n",
        "\n",
        "            # Early stopping check\n",
        "            if no_improve_epochs >= early_stopping_patience:\n",
        "                print(\"🛑 Early stopping triggered. Training stopped.\")\n",
        "                break\n",
        "\n",
        "            # Log results for this epoch\n",
        "            print(f\"\\nMaxPool - Val Loss: {val_loss_maxpool:.4f}, Val F1: {val_f1_maxpool:.4f}, Val Accuracy: {val_accuracy_maxpool:.4f}\")\n",
        "            print(f\"GlobalPool - Val Loss: {val_loss_globalpool:.4f}, Val F1: {val_f1_globalpool:.4f}, Val Accuracy: {val_accuracy_globalpool:.4f}\")\n",
        "\n",
        "        # Load best weights\n",
        "        if best_maxpool_state:\n",
        "            self.model_maxpool.load_state_dict(best_maxpool_state)\n",
        "        if best_globalpool_state:\n",
        "            self.model_globalpool.load_state_dict(best_globalpool_state)\n",
        "\n",
        "    def final_evaluation(self):\n",
        "        \"\"\"\n",
        "        Perform final evaluation on the test set for both models.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing confusion matrices and classification reports for both models.\n",
        "        \"\"\"\n",
        "        results = {}\n",
        "        for name, model in [('MaxPool', self.model_maxpool), ('GlobalPool', self.model_globalpool)]:\n",
        "            model.eval()\n",
        "            all_preds = []\n",
        "            all_labels = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in self.test_loader:\n",
        "                    input_ids = batch['input_ids'].to(self.device)\n",
        "                    attention_mask = batch['attention_mask'].to(self.device)\n",
        "                    features = batch['features'].to(self.device)\n",
        "                    labels = batch['labels'].to(self.device)\n",
        "\n",
        "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, features=features)\n",
        "                    preds = torch.argmax(outputs['logits'], dim=1)\n",
        "                    all_preds.extend(preds.detach().cpu().numpy())\n",
        "                    all_labels.extend(labels.detach().cpu().numpy())\n",
        "\n",
        "            results[name] = {\n",
        "                'confusion_matrix': confusion_matrix(all_labels, all_preds),\n",
        "                'classification_report': classification_report(all_labels, all_preds, output_dict=True)\n",
        "            }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def plot_comparison(self):\n",
        "        \"\"\"\n",
        "        Plot comparison of training and validation metrics for both models.\n",
        "\n",
        "        Returns:\n",
        "            Matplotlib figure object.\n",
        "        \"\"\"\n",
        "        sns.set_theme(style=\"whitegrid\", font_scale=1.2)\n",
        "        fig = plt.figure(figsize=(20, 20))\n",
        "        gs = fig.add_gridspec(4, 2)\n",
        "\n",
        "        # Training and Validation Loss\n",
        "        ax1 = fig.add_subplot(gs[0, 0])\n",
        "        epochs = range(1, len(self.results['MaxPool']['train_loss']) + 1)\n",
        "        ax1.plot(epochs, self.results['MaxPool']['train_loss'], 'b--', label='MaxPool (Train)')\n",
        "        ax1.plot(epochs, self.results['MaxPool']['val_loss'], 'b-', label='MaxPool (Val)')\n",
        "        ax1.plot(epochs, self.results['GlobalPool']['train_loss'], 'r--', label='GlobalPool (Train)')\n",
        "        ax1.plot(epochs, self.results['GlobalPool']['val_loss'], 'r-', label='GlobalPool (Val)')\n",
        "        ax1.set_title('Training and Validation Loss')\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.set_ylabel('Loss')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True)\n",
        "\n",
        "        # Validation F1 Score\n",
        "        ax2 = fig.add_subplot(gs[0, 1])\n",
        "        ax2.plot(epochs, self.results['MaxPool']['val_f1'], 'b-', label='MaxPool')\n",
        "        ax2.plot(epochs, self.results['GlobalPool']['val_f1'], 'r-', label='GlobalPool')\n",
        "        ax2.set_title('Validation F1 Score')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.set_ylabel('F1 Score')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True)\n",
        "\n",
        "        # Validation Accuracy\n",
        "        ax3 = fig.add_subplot(gs[1, 0])\n",
        "        ax3.plot(epochs, self.results['MaxPool']['val_accuracy'], 'b-', label='MaxPool')\n",
        "        ax3.plot(epochs, self.results['GlobalPool']['val_accuracy'], 'r-', label='GlobalPool')\n",
        "        ax3.set_title('Validation Accuracy')\n",
        "        ax3.set_xlabel('Epoch')\n",
        "        ax3.set_ylabel('Accuracy')\n",
        "        ax3.legend()\n",
        "        ax3.grid(True)\n",
        "\n",
        "        # Confusion Matrix for MaxPool\n",
        "        ax4 = fig.add_subplot(gs[1, 1])\n",
        "        final_results = self.final_evaluation()\n",
        "        sns.heatmap(final_results['MaxPool']['confusion_matrix'], annot=True, fmt='d', cmap='Blues', ax=ax4)\n",
        "        ax4.set_title('Confusion Matrix - MaxPool')\n",
        "        ax4.set_xlabel('Predicted')\n",
        "        ax4.set_ylabel('True')\n",
        "\n",
        "        # Confusion Matrix for GlobalPool\n",
        "        ax5 = fig.add_subplot(gs[2, 0])\n",
        "        sns.heatmap(final_results['GlobalPool']['confusion_matrix'], annot=True, fmt='d', cmap='Reds', ax=ax5)\n",
        "        ax5.set_title('Confusion Matrix - GlobalPool')\n",
        "        ax5.set_xlabel('Predicted')\n",
        "        ax5.set_ylabel('True')\n",
        "\n",
        "        # Performance Metrics Comparison\n",
        "        ax6 = fig.add_subplot(gs[2, 1])\n",
        "        metrics = ['precision', 'recall', 'f1-score', 'accuracy']\n",
        "        maxpool_scores = [final_results['MaxPool']['classification_report']['weighted avg'][m] for m in metrics[:-1]] + [final_results['MaxPool']['classification_report']['accuracy']]\n",
        "        globalpool_scores = [final_results['GlobalPool']['classification_report']['weighted avg'][m] for m in metrics[:-1]] + [final_results['GlobalPool']['classification_report']['accuracy']]\n",
        "        x = np.arange(len(metrics))\n",
        "        width = 0.35\n",
        "        ax6.bar(x - width/2, maxpool_scores, width, label='MaxPool', color='blue', alpha=0.6)\n",
        "        ax6.bar(x + width/2, globalpool_scores, width, label='GlobalPool', color='red', alpha=0.6)\n",
        "        ax6.set_title('Performance Metrics Comparison')\n",
        "        ax6.set_xticks(x)\n",
        "        ax6.set_xticklabels(metrics)\n",
        "        ax6.legend()\n",
        "\n",
        "        # Smoothed Training Loss Progress\n",
        "        ax7 = fig.add_subplot(gs[3, 0])\n",
        "        window_size = 5\n",
        "        maxpool_smooth = np.convolve(self.results['MaxPool']['train_loss'], np.ones(window_size)/window_size, mode='valid')\n",
        "        globalpool_smooth = np.convolve(self.results['GlobalPool']['train_loss'], np.ones(window_size)/window_size, mode='valid')\n",
        "        ax7.plot(maxpool_smooth, label='MaxPool')\n",
        "        ax7.plot(globalpool_smooth, label='GlobalPool')\n",
        "        ax7.set_title('Smoothed Training Loss Progress')\n",
        "        ax7.set_xlabel('Training Steps')\n",
        "        ax7.set_ylabel('Smoothed Loss')\n",
        "        ax7.legend()\n",
        "        ax7.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig"
      ],
      "metadata": {
        "id": "0vgRjg0xSdsU"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import wandb\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import (\n",
        "    f1_score, accuracy_score, confusion_matrix, classification_report,\n",
        "    precision_score, recall_score, roc_auc_score\n",
        ")\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "\n",
        "class ModelComparison:\n",
        "    def __init__(self, model_maxpool, model_globalpool, train_dataset, val_dataset, test_dataset, batch_size=16):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Set up models\n",
        "        self.model_maxpool = model_maxpool.to(self.device)\n",
        "        self.model_globalpool = model_globalpool.to(self.device)\n",
        "\n",
        "        # Enable PyTorch 2.0+ optimizations if available\n",
        "        if hasattr(torch, 'compile'):\n",
        "            self.model_maxpool = torch.compile(self.model_maxpool)\n",
        "            self.model_globalpool = torch.compile(self.model_globalpool)\n",
        "\n",
        "        # Mixed precision training\n",
        "        self.scaler = torch.amp.GradScaler()\n",
        "\n",
        "        # Batch size and gradient accumulation\n",
        "        self.batch_size = batch_size\n",
        "        self.accumulate_grad_batches = 4\n",
        "\n",
        "        # Data loaders with optimized settings\n",
        "        self.train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=2,\n",
        "            persistent_workers=True,\n",
        "            pin_memory=True,\n",
        "            drop_last=True\n",
        "        )\n",
        "        self.val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=4,\n",
        "            persistent_workers=True,\n",
        "            pin_memory=True\n",
        "        )\n",
        "        self.test_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=4,\n",
        "            persistent_workers=True,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        # Optimizers\n",
        "        self.optimizer = {\n",
        "            'MaxPool': torch.optim.AdamW(self.model_maxpool.parameters(), lr=3e-5, weight_decay=1e-4),\n",
        "            'GlobalPool': torch.optim.AdamW(self.model_globalpool.parameters(), lr=3e-5, weight_decay=1e-4)\n",
        "        }\n",
        "\n",
        "        # Learning rate schedulers with cosine annealing and warm restarts\n",
        "        total_steps = len(self.train_loader) * 5  # 5 epochs\n",
        "        warmup_steps = int(0.1 * total_steps)\n",
        "        self.scheduler = {\n",
        "            'MaxPool': torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "                self.optimizer['MaxPool'], T_0=10, T_mult=2, eta_min=1e-6\n",
        "            ),\n",
        "            'GlobalPool': torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "                self.optimizer['GlobalPool'], T_0=10, T_mult=2, eta_min=1e-6\n",
        "            )\n",
        "        }\n",
        "\n",
        "        # Logging setup\n",
        "        self.writer = SummaryWriter('runs/anxiety_detection_experiment')\n",
        "        wandb.init(project=\"anxiety-detection\", config={\n",
        "            \"batch_size\": batch_size,\n",
        "            \"learning_rate\": 3e-5,\n",
        "            \"architecture\": \"MaxPool_vs_GlobalPool\"\n",
        "        })\n",
        "\n",
        "        # Compute class weights for the entire dataset\n",
        "        all_labels = []\n",
        "        for batch in self.train_loader:\n",
        "            all_labels.extend(batch['labels'].cpu().numpy())\n",
        "        self.class_weights = compute_class_weight(\n",
        "            class_weight=\"balanced\",\n",
        "            classes=np.unique(all_labels),\n",
        "            y=all_labels\n",
        "        )\n",
        "        self.class_weights = torch.tensor(self.class_weights, dtype=torch.float32).to(self.device)\n",
        "\n",
        "        # Loss function with class weights\n",
        "        self.num_epochs = 5\n",
        "        self.criterion = nn.CrossEntropyLoss(weight=self.class_weights)\n",
        "\n",
        "        # Initialize results dictionary\n",
        "        self.results = {\n",
        "            'MaxPool': {'train_loss': [], 'val_loss': [], 'val_f1': [], 'val_accuracy': []},\n",
        "            'GlobalPool': {'train_loss': [], 'val_loss': [], 'val_f1': [], 'val_accuracy': []}\n",
        "        }\n",
        "\n",
        "    def train_epoch(self, model, optimizer, train_loader, accumulate_grad_batches, epoch):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1} Training\")\n",
        "\n",
        "        for batch_idx, batch in enumerate(progress_bar):\n",
        "            batch = {k: v.to(self.device, non_blocking=True) for k, v in batch.items()}\n",
        "\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                outputs = model(**batch)\n",
        "                loss = outputs['loss'].float() / accumulate_grad_batches\n",
        "\n",
        "            self.scaler.scale(loss).backward()\n",
        "\n",
        "            if (batch_idx + 1) % accumulate_grad_batches == 0 or (batch_idx + 1 == len(train_loader)):\n",
        "                self.scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                self.scaler.step(optimizer)\n",
        "                self.scaler.update()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            total_loss += loss.item() * accumulate_grad_batches\n",
        "\n",
        "            step = epoch * len(train_loader) + batch_idx\n",
        "            self.writer.add_scalar('Train/loss', loss.item(), step)\n",
        "            wandb.log({\n",
        "                \"Train/loss\": loss.item(),\n",
        "                \"step\": step,\n",
        "                \"epoch\": epoch,\n",
        "                \"learning_rate\": optimizer.param_groups[0]['lr']\n",
        "            })\n",
        "\n",
        "            progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        return avg_loss\n",
        "\n",
        "    def evaluate(self, model, data_loader):\n",
        "        model.eval()\n",
        "        total_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        all_probs = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in data_loader:\n",
        "                batch = {k: v.to(self.device, non_blocking=True) for k, v in batch.items()}\n",
        "\n",
        "                with torch.amp.autocast('cuda'):\n",
        "                    outputs = model(**batch)\n",
        "                    if 'loss' in outputs:\n",
        "                        total_loss += outputs['loss'].item()\n",
        "\n",
        "                logits = outputs['logits']\n",
        "                labels = batch['labels'].cpu().numpy()\n",
        "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "                probs = F.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
        "\n",
        "                all_preds.extend(preds)\n",
        "                all_labels.extend(labels)\n",
        "                all_probs.extend(probs)\n",
        "\n",
        "        unique_classes = np.unique(all_labels)\n",
        "        metrics = {\n",
        "            'loss': total_loss / len(data_loader),\n",
        "            'f1': f1_score(all_labels, all_preds, average='weighted'),\n",
        "            'accuracy': accuracy_score(all_labels, all_preds),\n",
        "            'precision': precision_score(all_labels, all_preds, average='weighted'),\n",
        "            'recall': recall_score(all_labels, all_preds, average='weighted')\n",
        "        }\n",
        "\n",
        "        if len(unique_classes) == 2:\n",
        "            metrics['roc_auc'] = roc_auc_score(all_labels, all_probs)\n",
        "        else:\n",
        "            metrics['roc_auc'] = 0.5\n",
        "\n",
        "        for metric_name, value in metrics.items():\n",
        "            self.writer.add_scalar(f'val/{metric_name}', value)\n",
        "            wandb.log({f\"val_{metric_name}\": value})\n",
        "\n",
        "        return metrics['loss'], metrics['f1'], metrics['roc_auc'], metrics['accuracy']\n",
        "\n",
        "    def train_and_compare(self):\n",
        "        early_stopping_patience = 3\n",
        "        best_val_loss = float('inf')\n",
        "        best_maxpool_state = None\n",
        "        best_globalpool_state = None\n",
        "        no_improve_epochs = 0\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            print(f\"\\nEpoch {epoch + 1}/{self.num_epochs}\")\n",
        "\n",
        "            train_loss_maxpool = self.train_epoch(\n",
        "                self.model_maxpool,\n",
        "                self.optimizer['MaxPool'],\n",
        "                self.train_loader,\n",
        "                self.accumulate_grad_batches,\n",
        "                epoch\n",
        "            )\n",
        "            val_loss_maxpool, val_f1_maxpool, _, _ = self.evaluate(\n",
        "                self.model_maxpool,\n",
        "                self.val_loader\n",
        "            )\n",
        "\n",
        "            self.results['MaxPool']['train_loss'].append(train_loss_maxpool)\n",
        "            self.results['MaxPool']['val_loss'].append(val_loss_maxpool)\n",
        "            self.results['MaxPool']['val_f1'].append(val_f1_maxpool)\n",
        "\n",
        "            if val_loss_maxpool < best_val_loss:\n",
        "                best_val_loss = val_loss_maxpool\n",
        "                best_maxpool_state = self.model_maxpool.state_dict().copy()\n",
        "                no_improve_epochs = 0\n",
        "\n",
        "            self.scheduler['MaxPool'].step()\n",
        "\n",
        "            train_loss_globalpool = self.train_epoch(\n",
        "                self.model_globalpool,\n",
        "                self.optimizer['GlobalPool'],\n",
        "                self.train_loader,\n",
        "                self.accumulate_grad_batches,\n",
        "                epoch\n",
        "            )\n",
        "            val_loss_globalpool, val_f1_globalpool, _, _ = self.evaluate(\n",
        "                self.model_globalpool,\n",
        "                self.val_loader\n",
        "            )\n",
        "\n",
        "            self.results['GlobalPool']['train_loss'].append(train_loss_globalpool)\n",
        "            self.results['GlobalPool']['val_loss'].append(val_loss_globalpool)\n",
        "            self.results['GlobalPool']['val_f1'].append(val_f1_globalpool)\n",
        "\n",
        "            if val_loss_globalpool < best_val_loss:\n",
        "                best_val_loss = val_loss_globalpool\n",
        "                best_globalpool_state = copy.deepcopy(self.model_globalpool.state_dict())\n",
        "                no_improve_epochs = 0\n",
        "            else:\n",
        "                no_improve_epochs += 1\n",
        "\n",
        "            self.scheduler['GlobalPool'].step()\n",
        "\n",
        "            if no_improve_epochs >= early_stopping_patience:\n",
        "                print(\"🛑 Early stopping triggered. Training stopped.\")\n",
        "                break\n",
        "\n",
        "            print(f\"\\nMaxPool - Val Loss: {val_loss_maxpool:.4f}, Val F1: {val_f1_maxpool:.4f}\")\n",
        "            print(f\"GlobalPool - Val Loss: {val_loss_globalpool:.4f}, Val F1: {val_f1_globalpool:.4f}\")\n",
        "\n",
        "        if best_maxpool_state:\n",
        "            self.model_maxpool.load_state_dict(best_maxpool_state)\n",
        "        if best_globalpool_state:\n",
        "            self.model_globalpool.load_state_dict(best_globalpool_state)\n",
        "\n",
        "    def final_evaluation(self):\n",
        "        results = {}\n",
        "        for name, model in [('MaxPool', self.model_maxpool), ('GlobalPool', self.model_globalpool)]:\n",
        "            model.eval()\n",
        "            all_preds = []\n",
        "            all_labels = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in self.test_loader:\n",
        "                    input_ids = batch['input_ids'].to(self.device)\n",
        "                    attention_mask = batch['attention_mask'].to(self.device)\n",
        "                    labels = batch['labels'].to(self.device)\n",
        "\n",
        "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)  # حذف features\n",
        "                    preds = torch.argmax(outputs['logits'], dim=1)\n",
        "                    all_preds.extend(preds.detach().cpu().numpy())\n",
        "                    all_labels.extend(labels.detach().cpu().numpy())\n",
        "\n",
        "            results[name] = {\n",
        "                'confusion_matrix': confusion_matrix(all_labels, all_preds),\n",
        "                'classification_report': classification_report(all_labels, all_preds, output_dict=True)\n",
        "            }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def plot_comparison(self):\n",
        "        sns.set_theme(style=\"whitegrid\", font_scale=1.2)\n",
        "        fig = plt.figure(figsize=(20, 16))\n",
        "        gs = fig.add_gridspec(4, 2)\n",
        "\n",
        "        ax1 = fig.add_subplot(gs[0, 0])\n",
        "        epochs = range(1, len(self.results['MaxPool']['train_loss']) + 1)\n",
        "        ax1.plot(epochs, self.results['MaxPool']['train_loss'], 'b--', label='MaxPool (Train)')\n",
        "        ax1.plot(epochs, self.results['MaxPool']['val_loss'], 'b-', label='MaxPool (Val)')\n",
        "        ax1.plot(epochs, self.results['GlobalPool']['train_loss'], 'r--', label='GlobalPool (Train)')\n",
        "        ax1.plot(epochs, self.results['GlobalPool']['val_loss'], 'r-', label='GlobalPool (Val)')\n",
        "        ax1.set_title('Training and Validation Loss')\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.set_ylabel('Loss')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True)\n",
        "\n",
        "        ax2 = fig.add_subplot(gs[0, 1])\n",
        "        ax2.plot(epochs, self.results['MaxPool']['val_f1'], 'b-', label='MaxPool')\n",
        "        ax2.plot(epochs, self.results['GlobalPool']['val_f1'], 'r-', label='GlobalPool')\n",
        "        ax2.set_title('Validation F1 Score')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.set_ylabel('F1 Score')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True)\n",
        "\n",
        "        ax3 = fig.add_subplot(gs[1, 0])\n",
        "        ax3.plot(epochs, self.results['MaxPool']['val_accuracy'], 'b-', label='MaxPool')\n",
        "        ax3.plot(epochs, self.results['GlobalPool']['val_accuracy'], 'r-', label='GlobalPool')\n",
        "        ax3.set_title('Validation Accuracy')\n",
        "        ax3.set_xlabel('Epoch')\n",
        "        ax3.set_ylabel('Accuracy')\n",
        "        ax3.legend()\n",
        "        ax3.grid(True)\n",
        "\n",
        "        ax4 = fig.add_subplot(gs[1, 1])\n",
        "        final_results = self.final_evaluation()\n",
        "        sns.heatmap(final_results['MaxPool']['confusion_matrix'], annot=True, fmt='d', cmap='Blues', ax=ax4)\n",
        "        ax4.set_title('Confusion Matrix - MaxPool')\n",
        "        ax4.set_xlabel('Predicted')\n",
        "        ax4.set_ylabel('Accuracy')\n",
        "\n",
        "        ax5 = fig.add_subplot(gs[2, 0])\n",
        "        sns.heatmap(final_results['GlobalPool']['confusion_matrix'], annot=True, fmt='d', cmap='Reds', ax=ax5)\n",
        "        ax5.set_title('Confusion Matrix - GlobalPool')\n",
        "        ax5.set_xlabel('Predicted')\n",
        "        ax5.set_ylabel('True')\n",
        "\n",
        "        ax6 = fig.add_subplot(gs[2, 1])\n",
        "        metrics = ['precision', 'recall', 'f1-score', 'accuracy']\n",
        "        maxpool_scores = [final_results['MaxPool']['classification_report']['weighted avg'][m] for m in metrics[:-1]] + [final_results['MaxPool']['classification_report']['accuracy']]\n",
        "        globalpool_scores = [final_results['GlobalPool']['classification_report']['weighted avg'][m] for m in metrics[:-1]] + [final_results['GlobalPool']['classification_report']['accuracy']]\n",
        "        x = np.arange(len(metrics))\n",
        "        width = 0.35\n",
        "        ax6.bar(x - width/2, maxpool_scores, width, label='MaxPool', color='blue', alpha=0.6)\n",
        "        ax6.bar(x + width/2, globalpool_scores, width, label='GlobalPool', color='red', alpha=0.6)\n",
        "        ax6.set_title('Performance Metrics Comparison')\n",
        "        ax6.set_xticks(x)\n",
        "        ax6.set_xticklabels(metrics)\n",
        "        ax6.legend()\n",
        "\n",
        "        ax7 = fig.add_subplot(gs[3, 0])\n",
        "        window_size = 5\n",
        "        maxpool_smooth = np.convolve(np.ones(window_size)/window_size, self.results['MaxPool']['train_loss'], mode='valid')\n",
        "        globalpool_smooth = np.convolve(np.ones(window_size)/window_size, self.results['GlobalPool']['train_loss'], mode='valid')\n",
        "        ax7.plot(maxpool_smooth, label='MaxPool')\n",
        "        ax7.plot(globalpool_smooth, label='GlobalPool')\n",
        "        ax7.set_title('Smoothed Training Loss Progress')\n",
        "        ax7.set_xlabel('Step')\n",
        "        ax7.set_ylabel('Smoothed Loss')\n",
        "        ax7.legend()\n",
        "        ax7.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n"
      ],
      "metadata": {
        "id": "clbU6rXQcveU"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# تابع convert_to_serializable را خارج از کلاس تعریف کنید\n",
        "def convert_to_serializable(obj):\n",
        "    \"\"\"\n",
        "    تبدیل تنسورها و آرایه‌های نامطلوب به مقادیر قابل سریالیزه\n",
        "    \"\"\"\n",
        "    if isinstance(obj, torch.Tensor):\n",
        "        return obj.cpu().detach().numpy().tolist()\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    elif isinstance(obj, dict):\n",
        "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [convert_to_serializable(v) for v in obj]\n",
        "    elif isinstance(obj, tuple):\n",
        "        return tuple(convert_to_serializable(v) for v in obj)\n",
        "    elif hasattr(obj, 'item') and callable(getattr(obj, 'item')):\n",
        "        return obj.item()\n",
        "    elif isinstance(obj, (np.int32, np.int64, np.float32, np.float64)):\n",
        "        return obj.item()\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "class StratifiedKFoldTrainer:\n",
        "    def __init__(self, n_splits=5, random_state=42):\n",
        "        self.n_splits = n_splits\n",
        "        self.random_state = random_state\n",
        "        self.device = torch.device('cuda')\n",
        "        self.writer = SummaryWriter('runs/anxiety_detection_kfold')\n",
        "        self.scaler = torch.amp.GradScaler()\n",
        "\n",
        "    def create_fold_datasets(self, df, tokenizer, fold_idx, train_idx, val_idx):\n",
        "        \"\"\"Create training and validation datasets for a specific fold\"\"\"\n",
        "        train_data = df.iloc[train_idx]\n",
        "        val_data = df.iloc[val_idx]\n",
        "\n",
        "        train_dataset = AnxietyDataset(\n",
        "            train_data['text'].values,\n",
        "            train_data['label'].values,\n",
        "            tokenizer\n",
        "        )\n",
        "        val_dataset = AnxietyDataset(\n",
        "            val_data['text'].values,\n",
        "            val_data['label'].values,\n",
        "            tokenizer\n",
        "        )\n",
        "\n",
        "        return train_dataset, val_dataset\n",
        "\n",
        "    def train_fold(self, model, train_dataset, val_dataset, fold_idx):\n",
        "        \"\"\"Train model on a specific fold\"\"\"\n",
        "        train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, pin_memory=True, persistent_workers=True, num_workers=2, drop_last=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=8)\n",
        "\n",
        "        # Calculate class weights for this fold\n",
        "        all_labels = []\n",
        "        for batch in train_loader:\n",
        "            all_labels.extend(batch['labels'].cpu().numpy().tolist())\n",
        "\n",
        "        class_weights = compute_class_weight(\n",
        "            class_weight=\"balanced\",\n",
        "            classes=np.unique(all_labels),\n",
        "            y=all_labels\n",
        "        )\n",
        "        class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.device)\n",
        "\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='min', factor=0.5, patience=2\n",
        "        )\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        early_stopping_counter = 0\n",
        "        early_stopping_patience = 3\n",
        "        fold_results = {\n",
        "            'train_loss': [], 'val_loss': [], 'val_f1': [],\n",
        "            'best_model_state': None, 'best_val_metrics': None\n",
        "        }\n",
        "\n",
        "        for epoch in range(2):  # 5 epochs per fold\n",
        "            # Training\n",
        "            model.train()\n",
        "            total_train_loss = 0\n",
        "\n",
        "            for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Fold {fold_idx+1} Epoch {epoch+1}\")):\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                batch = {k: v.to(self.device, non_blocking=True) for k, v in batch.items()}\n",
        "\n",
        "                # Forward pass with mixed precision\n",
        "                with torch.amp.autocast(device_type='cuda'):\n",
        "                    outputs = model(**batch)\n",
        "                    loss = outputs['loss']\n",
        "\n",
        "                # Backward pass with gradient scaling\n",
        "                self.scaler.scale(loss).backward()\n",
        "                self.scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                self.scaler.step(optimizer)\n",
        "                self.scaler.update()\n",
        "\n",
        "                total_train_loss += loss.item()\n",
        "\n",
        "                # Log metrics\n",
        "                self.writer.add_scalar(\n",
        "                    f'Fold_{fold_idx+1}/train_loss',\n",
        "                    loss.item(),\n",
        "                    epoch * len(train_loader) + batch_idx\n",
        "                )\n",
        "\n",
        "            avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "            # Validation\n",
        "            model.eval()\n",
        "            val_metrics = self.evaluate_model(model, val_loader)\n",
        "\n",
        "            # Update learning rate\n",
        "            scheduler.step(val_metrics['loss'])\n",
        "\n",
        "            # Early stopping check\n",
        "            if val_metrics['loss'] < best_val_loss:\n",
        "                best_val_loss = val_metrics['loss']\n",
        "                fold_results['best_model_state'] = copy.deepcopy(model.state_dict())\n",
        "                fold_results['best_val_metrics'] = val_metrics\n",
        "                early_stopping_counter = 0\n",
        "            else:\n",
        "                early_stopping_counter += 1\n",
        "\n",
        "            if early_stopping_counter >= early_stopping_patience:\n",
        "                print(f\"Early stopping triggered in fold {fold_idx+1}\")\n",
        "                break\n",
        "\n",
        "            # Store results\n",
        "            fold_results['train_loss'].append(avg_train_loss)\n",
        "            fold_results['val_loss'].append(val_metrics['loss'])\n",
        "            fold_results['val_f1'].append(val_metrics['f1'])\n",
        "\n",
        "            # Log fold metrics\n",
        "            for metric_name, value in val_metrics.items():\n",
        "                self.writer.add_scalar(\n",
        "                    f'Fold_{fold_idx+1}/val_{metric_name}',\n",
        "                    value,\n",
        "                    epoch\n",
        "                )\n",
        "\n",
        "        return fold_results\n",
        "\n",
        "    def evaluate_model(self, model, data_loader):\n",
        "        \"\"\"Evaluate model performance\"\"\"\n",
        "        model.eval()\n",
        "        total_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        all_probs = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in data_loader:\n",
        "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
        "                outputs = model(**batch)\n",
        "\n",
        "                loss = outputs['loss']\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                logits = outputs['logits']\n",
        "                probs = F.softmax(logits, dim=1)\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(batch['labels'].cpu().numpy())\n",
        "                all_probs.extend(probs[:, 1].cpu().numpy())\n",
        "\n",
        "        # بررسی تعداد کلاس‌های موجود در `all_labels`\n",
        "        unique_classes = np.unique(all_labels)\n",
        "\n",
        "        if len(unique_classes) == 2:\n",
        "            # برای دو کلاس، `multi_class` نیاز نیست\n",
        "            roc_auc = roc_auc_score(all_labels, all_probs)\n",
        "        else:\n",
        "            # برای چندین کلاس، `multi_class='ovr'`\n",
        "            roc_auc = roc_auc_score(all_labels, all_probs, multi_class='ovr')\n",
        "\n",
        "        metrics = {\n",
        "            'loss': total_loss / len(data_loader),\n",
        "            'f1': f1_score(all_labels, all_preds, average='weighted'),\n",
        "            'accuracy': accuracy_score(all_labels, all_preds),\n",
        "            'precision': precision_score(all_labels, all_preds, average='weighted'),\n",
        "            'recall': recall_score(all_labels, all_preds, average='weighted'),\n",
        "            'roc_auc': roc_auc\n",
        "        }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def calculate_average_metrics(self, fold_results):\n",
        "        \"\"\"Calculate average metrics across all folds\"\"\"\n",
        "        all_metrics = defaultdict(list)\n",
        "\n",
        "        for fold_result in fold_results:\n",
        "            for metric, value in fold_result['best_val_metrics'].items():\n",
        "                # تبدیل تنسورها به اعداد float\n",
        "                if isinstance(value, torch.Tensor):\n",
        "                    value = value.detach().cpu().numpy().item()\n",
        "                elif isinstance(value, np.ndarray):\n",
        "                    value = value.item() if value.size == 1 else value.tolist()\n",
        "                elif isinstance(value, (np.float32, np.float64, np.int32, np.int64)):\n",
        "                    value = value.item()\n",
        "\n",
        "                all_metrics[metric].append(value)\n",
        "\n",
        "        avg_metrics = {\n",
        "            metric: {\n",
        "                'mean': float(np.mean(values)),\n",
        "                'std': float(np.std(values))\n",
        "            }\n",
        "            for metric, values in all_metrics.items()\n",
        "        }\n",
        "\n",
        "        return avg_metrics\n",
        "\n",
        "    def log_final_results(self, avg_metrics):\n",
        "        \"\"\"Log final results and create visualizations\"\"\"\n",
        "        print(\"\\nFinal Cross-Validation Results:\")\n",
        "        for metric, stats in avg_metrics.items():\n",
        "            print(f\"{metric}:\")\n",
        "            print(f\"  Mean: {stats['mean']:.4f}\")\n",
        "            print(f\"  Std:  {stats['std']:.4f}\")\n",
        "\n",
        "            # تبدیل به مقادیر عددی قبل از لاگ\n",
        "            mean_val = float(stats['mean'])\n",
        "            std_val = float(stats['std'])\n",
        "\n",
        "            self.writer.add_scalar(f'Final/avg_{metric}', mean_val)\n",
        "            self.writer.add_scalar(f'Final/std_{metric}', std_val)\n",
        "\n",
        "    def train_with_kfold(self, df, tokenizer, model_class):\n",
        "        \"\"\"Perform k-fold cross validation training\"\"\"\n",
        "        skf = StratifiedKFold(\n",
        "            n_splits=self.n_splits,\n",
        "            shuffle=True,\n",
        "            random_state=self.random_state\n",
        "        )\n",
        "\n",
        "        fold_results = []\n",
        "        overall_best_model = None\n",
        "        best_val_f1 = 0\n",
        "\n",
        "        for fold_idx, (train_idx, val_idx) in enumerate(skf.split(df, df['label'])):\n",
        "            print(f\"\\nTraining Fold {fold_idx + 1}/{self.n_splits}\")\n",
        "\n",
        "            # Create datasets for this fold\n",
        "            train_dataset, val_dataset = self.create_fold_datasets(\n",
        "                df, tokenizer, fold_idx, train_idx, val_idx\n",
        "            )\n",
        "\n",
        "            # Initialize a new model for this fold\n",
        "            config = AutoConfig.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
        "            if model_class == GlobalPoolCNNModel:\n",
        "                model = model_class(config, train_labels=df['label'].values).to(self.device)\n",
        "            else:\n",
        "                model = model_class(config).to(self.device)\n",
        "\n",
        "            # Train the model on this fold\n",
        "            fold_result = self.train_fold(model, train_dataset, val_dataset, fold_idx)\n",
        "\n",
        "            # تبدیل نتایج fold به فرمت قابل سریالیزه قبل از اضافه کردن به لیست\n",
        "            serializable_fold_result = {\n",
        "                'train_loss': [float(loss) for loss in fold_result['train_loss']],\n",
        "                'val_loss': [float(loss) for loss in fold_result['val_loss']],\n",
        "                'val_f1': [float(f1) for f1 in fold_result['val_f1']],\n",
        "                'best_val_metrics': convert_to_serializable(fold_result['best_val_metrics'])\n",
        "                # best_model_state را نگه نمیداریم چون نیازی به سریالیزه کردن آن نیست\n",
        "            }\n",
        "\n",
        "            fold_results.append(serializable_fold_result)\n",
        "\n",
        "            # Update best model if needed\n",
        "            if fold_result['best_val_metrics']['f1'] > best_val_f1:\n",
        "                best_val_f1 = fold_result['best_val_metrics']['f1']\n",
        "                overall_best_model = copy.deepcopy(fold_result['best_model_state'])\n",
        "\n",
        "        # Calculate and log average metrics across folds\n",
        "        avg_metrics = self.calculate_average_metrics(fold_results)\n",
        "        self.log_final_results(avg_metrics)\n",
        "\n",
        "        # تبدیل نتایج نهایی به فرمت قابل سریالیزه\n",
        "        kfold_results = {\n",
        "            'metrics': convert_to_serializable(avg_metrics),\n",
        "            'fold_results': fold_results\n",
        "        }\n",
        "\n",
        "        os.makedirs(\"experiment_results/kfold\", exist_ok=True)\n",
        "        with open(os.path.join(\"experiment_results/kfold\", f'cross_validation_results_{model_class.__name__}.json'), 'w') as f:\n",
        "            json.dump(kfold_results, f, indent=2)\n",
        "\n",
        "        return overall_best_model, avg_metrics, fold_results\n",
        "\n"
      ],
      "metadata": {
        "id": "7wNE8UrcSlLC"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pT8_6gmfgIDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import wandb\n",
        "from transformers import DistilBertTokenizer, AutoConfig\n",
        "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "\n",
        "# Setup\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "nltk_data_path = os.path.join(os.getcwd(), 'nltk_data')\n",
        "os.makedirs(nltk_data_path, exist_ok=True)\n",
        "nltk.data.path.append(nltk_data_path)\n",
        "nltk.download('wordnet', download_dir=nltk_data_path)\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Convert to serializable\n",
        "def convert_to_serializable(obj):\n",
        "    if isinstance(obj, torch.Tensor):\n",
        "        return obj.cpu().detach().numpy().tolist()\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    elif isinstance(obj, dict):\n",
        "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [convert_to_serializable(v) for v in obj]\n",
        "    elif isinstance(obj, tuple):\n",
        "        return tuple(convert_to_serializable(v) for v in obj)\n",
        "    elif hasattr(obj, 'item') and callable(getattr(obj, 'item')):\n",
        "        return obj.item()\n",
        "    elif isinstance(obj, (np.int32, np.int64, np.float32, np.float64)):\n",
        "        return obj.item()\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "# Dataset\n",
        "class AnxietyDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Load and preprocess data\n",
        "def load_and_preprocess_data(filename, augment=True):\n",
        "    logger.info(f\"Loading {filename}...\")\n",
        "    df = pd.read_csv(filename)\n",
        "\n",
        "    # Validate required columns\n",
        "    required_columns = ['text', 'label']\n",
        "    if not all(col in df.columns for col in required_columns):\n",
        "        missing = [col for col in required_columns if col not in df.columns]\n",
        "        raise ValueError(f\"Missing columns in {filename}: {missing}\")\n",
        "\n",
        "    # Augmentation\n",
        "    if augment:\n",
        "        augmenter = AdvancedDataAugmenter(alpha_sr=0.1, alpha_rd=0.1, alpha_ri=0.1, alpha_rs=0.1, num_aug=2)\n",
        "        augmented_texts, augmented_labels = [], []\n",
        "        for _, row in df.iterrows():\n",
        "            text, label = row['text'], row['label']\n",
        "            if random.random() < 0.5:\n",
        "                aug_texts = augmenter.augment_text(text, mode=\"train\")\n",
        "                augmented_texts.extend(aug_texts)\n",
        "                augmented_labels.extend([label] * len(aug_texts))\n",
        "            else:\n",
        "                augmented_texts.append(text)\n",
        "                augmented_labels.append(label)\n",
        "        df = pd.DataFrame({'text': augmented_texts, 'label': augmented_labels})\n",
        "\n",
        "    return df\n",
        "\n",
        "# Evaluate model on test set\n",
        "def evaluate_test_set(model, test_dataset, device, batch_size=8):\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=2)\n",
        "    model.eval()\n",
        "    all_preds, all_labels, all_probs = [], [], []\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Evaluating Test Set\"):\n",
        "            batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
        "            with torch.amp.autocast(device_type='cuda' if device.type == 'cuda' else 'cpu'):\n",
        "                outputs = model(**batch)\n",
        "                if 'loss' in outputs:\n",
        "                    total_loss += outputs['loss'].item()\n",
        "            logits = outputs['logits']\n",
        "            probs = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
        "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(batch['labels'].cpu().numpy())\n",
        "            all_probs.extend(probs)\n",
        "\n",
        "    unique_classes = np.unique(all_labels)\n",
        "    metrics = {\n",
        "        'loss': total_loss / len(test_loader),\n",
        "        'f1': f1_score(all_labels, all_preds, average='weighted'),\n",
        "        'accuracy': accuracy_score(all_labels, all_preds),\n",
        "        'precision': precision_score(all_labels, all_preds, average='weighted'),\n",
        "        'recall': recall_score(all_labels, all_preds, average='weighted'),\n",
        "        'roc_auc': roc_auc_score(all_labels, all_probs) if len(unique_classes) == 2 else None\n",
        "    }\n",
        "    metrics = convert_to_serializable(metrics)\n",
        "    return metrics, confusion_matrix(all_labels, all_preds), classification_report(all_labels, all_preds, output_dict=True)\n",
        "\n",
        "def main():\n",
        "    set_seed(42)\n",
        "    max_length = 128\n",
        "    batch_size = 8\n",
        "    n_splits = 5\n",
        "    num_epochs = 5\n",
        "\n",
        "    # Initialize WandB\n",
        "    wandb.init(project=\"dreaddit-stress-detection\", config={\n",
        "        \"batch_size\": batch_size,\n",
        "        \"max_length\": max_length,\n",
        "        \"n_splits\": n_splits,\n",
        "        \"num_epochs\": num_epochs\n",
        "    })\n",
        "\n",
        "    # Load datasets\n",
        "    logger.info(\"Uploading dreaddit-train.csv...\")\n",
        "    uploaded = files.upload()\n",
        "    train_df = load_and_preprocess_data('dreaddit-train.csv', augment=True)\n",
        "\n",
        "    logger.info(\"Uploading dreaddit-test (3).csv...\")\n",
        "    uploaded = files.upload()\n",
        "    test_df = load_and_preprocess_data('dreaddit-test (3).csv', augment=False)\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "    # Initialize k-fold trainer\n",
        "    kfold_trainer = StratifiedKFoldTrainer(n_splits=n_splits, random_state=42)\n",
        "\n",
        "    # Train and evaluate both models\n",
        "    models = {\n",
        "        'MaxPool': MaxPoolCNNModel,\n",
        "        'GlobalPool': GlobalPoolCNNModel\n",
        "    }\n",
        "    test_results = {}\n",
        "    best_models = {}\n",
        "    kfold_metrics = {}\n",
        "\n",
        "    for model_name, model_class in models.items():\n",
        "        logger.info(f\"\\nTraining {model_name} with {n_splits}-fold cross-validation...\")\n",
        "        best_model_state, avg_metrics, fold_results = kfold_trainer.train_with_kfold(\n",
        "            train_df, tokenizer, model_class\n",
        "        )\n",
        "        kfold_metrics[model_name] = avg_metrics\n",
        "\n",
        "        # Initialize model with best weights\n",
        "        config = AutoConfig.from_pretrained(\"distilbert-base-uncased\")\n",
        "        model = model_class(config, max_length=max_length, train_labels=train_df['label'].values).to(kfold_trainer.device)\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "        # Evaluate on test set\n",
        "        test_dataset = AnxietyDataset(\n",
        "            test_df['text'].values,\n",
        "            test_df['label'].values,\n",
        "            tokenizer,\n",
        "            max_length\n",
        "        )\n",
        "        logger.info(f\"Evaluating {model_name} on dreaddit-test (3).csv...\")\n",
        "        test_metrics, test_cm, test_cr = evaluate_test_set(model, test_dataset, kfold_trainer.device, batch_size)\n",
        "        test_results[model_name] = {\n",
        "            'metrics': test_metrics,\n",
        "            'confusion_matrix': test_cm.tolist(),\n",
        "            'classification_report': test_cr\n",
        "        }\n",
        "        best_models[model_name] = best_model_state\n",
        "\n",
        "        # Log test results\n",
        "        logger.info(f\"\\n{model_name} Test Results:\")\n",
        "        logger.info(f\"Metrics: {json.dumps(test_metrics, indent=2)}\")\n",
        "        logger.info(f\"Confusion Matrix:\\n{test_cm}\")\n",
        "        logger.info(f\"Classification Report:\\n{json.dumps(test_cr, indent=2)}\")\n",
        "\n",
        "        # Log to WandB\n",
        "        wandb.log({f\"test_{model_name}_{k}\": v for k, v in test_metrics.items()})\n",
        "\n",
        "        # Save results\n",
        "        os.makedirs(\"experiment_results/test\", exist_ok=True)\n",
        "        with open(f\"experiment_results/test/{model_name}_test_results.json\", 'w') as f:\n",
        "            json.dump(test_results[model_name], f, indent=2)\n",
        "\n",
        "    # Model comparison\n",
        "    logger.info(\"\\nComparing models...\")\n",
        "    train_dataset = AnxietyDataset(\n",
        "        train_df['text'].values, train_df['label'].values, tokenizer, max_length\n",
        "    )\n",
        "    val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
        "        test_df['text'].values, test_df['label'].values,\n",
        "        test_size=0.5, random_state=42\n",
        "    )\n",
        "    val_dataset = AnxietyDataset(val_texts, val_labels, tokenizer, max_length)\n",
        "    test_dataset = AnxietyDataset(test_texts, test_labels, tokenizer, max_length)\n",
        "\n",
        "    config = AutoConfig.from_pretrained(\"distilbert-base-uncased\")\n",
        "    model_maxpool = MaxPoolCNNModel(config, max_length=max_length, train_labels=train_df['label'].values).to(kfold_trainer.device)\n",
        "    model_globalpool = GlobalPoolCNNModel(config, max_length=max_length, train_labels=train_df['label'].values).to(kfold_trainer.device)\n",
        "    model_maxpool.load_state_dict(best_models['MaxPool'])\n",
        "    model_globalpool.load_state_dict(best_models['GlobalPool'])\n",
        "\n",
        "    comparison = ModelComparison(\n",
        "        model_maxpool, model_globalpool, train_dataset, val_dataset, test_dataset, batch_size=batch_size\n",
        "    )\n",
        "    comparison.train_and_compare()\n",
        "\n",
        "    # Log comparison results\n",
        "    final_results = comparison.final_evaluation()\n",
        "    for name, result in final_results.items():\n",
        "        logger.info(f\"\\n{name} Final Test Results:\")\n",
        "        logger.info(f\"Confusion Matrix:\\n{result['confusion_matrix']}\")\n",
        "        logger.info(f\"Classification Report:\\n{json.dumps(result['classification_report'], indent=2)}\")\n",
        "        wandb.log({f\"final_{name}_{k}\": v for k, v in result['classification_report']['weighted avg'].items()})\n",
        "\n",
        "    # Save comparison results\n",
        "    with open(\"experiment_results/comparison_results.json\", 'w') as f:\n",
        "        json.dump(convert_to_serializable(final_results), f, indent=2)\n",
        "\n",
        "    # Visualizations\n",
        "    # 1. Training vs. Validation Loss (Overfitting/Underfitting)\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    for model_name in ['MaxPool', 'GlobalPool']:\n",
        "        train_loss = comparison.results[model_name]['train_loss']\n",
        "        val_loss = comparison.results[model_name]['val_loss']\n",
        "        epochs = range(1, len(train_loss) + 1)\n",
        "        ax1.plot(epochs, train_loss, label=f'{model_name} Train', linestyle='--')\n",
        "        ax1.plot(epochs, val_loss, label=f'{model_name} Val')\n",
        "    ax1.set_title('Training vs. Validation Loss')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # 2. Test Metrics Comparison\n",
        "    metrics = ['f1', 'accuracy', 'roc_auc']\n",
        "    maxpool_scores = [test_results['MaxPool']['metrics'][m] for m in metrics if test_results['MaxPool']['metrics'][m] is not None]\n",
        "    globalpool_scores = [test_results['GlobalPool']['metrics'][m] for m in metrics if test_results['GlobalPool']['metrics'][m] is not None]\n",
        "    valid_metrics = [m for m in metrics if test_results['MaxPool']['metrics'][m] is not None]\n",
        "\n",
        "    x = np.arange(len(valid_metrics))\n",
        "    width = 0.35\n",
        "    ax2.bar(x - width/2, maxpool_scores, width, label='MaxPool', color='blue', alpha=0.6)\n",
        "    ax2.bar(x + width/2, globalpool_scores, width, label='GlobalPool', color='red', alpha=0.6)\n",
        "    ax2.set_title('Test Metrics Comparison')\n",
        "    ax2.set_xticks(x)\n",
        "    ax2.set_xticklabels([m.replace('_', ' ').title() for m in valid_metrics])\n",
        "    ax2.set_ylabel('Score')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, axis='y')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    wandb.log({\"loss_plot\": wandb.Image(fig)})\n",
        "\n",
        "    # Overfitting/Underfitting Analysis\n",
        "    for model_name in ['MaxPool', 'GlobalPool']:\n",
        "        train_loss = comparison.results[model_name]['train_loss']\n",
        "        val_loss = comparison.results[model_name]['val_loss']\n",
        "        val_f1 = comparison.results[model_name]['val_f1']\n",
        "\n",
        "        logger.info(f\"\\n{model_name} Overfitting/Underfitting Analysis:\")\n",
        "        if train_loss[-1] < val_loss[-1] - 0.1 and val_f1[-1] < 0.7:\n",
        "            logger.warning(f\"{model_name} shows signs of overfitting: Train Loss ({train_loss[-1]:.4f}) << Val Loss ({val_loss[-1]:.4f}), Val F1 ({val_f1[-1]:.4f})\")\n",
        "        elif train_loss[-1] > 0.5 and val_f1[-1] < 0.6:\n",
        "            logger.warning(f\"{model_name} shows signs of underfitting: High Train Loss ({train_loss[-1]:.4f}), Low Val F1 ({val_f1[-1]:.4f})\")\n",
        "        else:\n",
        "            logger.info(f\"{model_name} appears well-fitted: Train Loss ({train_loss[-1]:.4f}), Val Loss ({val_loss[-1]:.4f}), Val F1 ({val_f1[-1]:.4f})\")\n",
        "\n",
        "    # Model Comparison\n",
        "    logger.info(\"\\nModel Comparison Summary:\")\n",
        "    for metric in ['f1', 'accuracy', 'roc_auc']:\n",
        "        if test_results['MaxPool']['metrics'][metric] is not None:\n",
        "            maxpool_val = test_results['MaxPool']['metrics'][metric]\n",
        "            globalpool_val = test_results['GlobalPool']['metrics'][metric]\n",
        "            logger.info(f\"{metric.upper()}: MaxPool = {maxpool_val:.4f}, GlobalPool = {globalpool_val:.4f}\")\n",
        "            if maxpool_val > globalpool_val:\n",
        "                logger.info(f\"MaxPool outperforms GlobalPool in {metric}\")\n",
        "            elif globalpool_val > maxpool_val:\n",
        "                logger.info(f\"GlobalPool outperforms MaxPool in {metric}\")\n",
        "            else:\n",
        "                logger.info(f\"Both models perform equally in {metric}\")\n",
        "\n",
        "def run():\n",
        "    main()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vG1gqoGsFy8x",
        "outputId": "7e434a41-800c-4cce-aac2-6ef3e3d3d22d"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /content/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_MaxPool_accuracy</td><td>▁</td></tr><tr><td>test_MaxPool_f1</td><td>▁</td></tr><tr><td>test_MaxPool_loss</td><td>▁</td></tr><tr><td>test_MaxPool_precision</td><td>▁</td></tr><tr><td>test_MaxPool_recall</td><td>▁</td></tr><tr><td>test_MaxPool_roc_auc</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_MaxPool_accuracy</td><td>0.51608</td></tr><tr><td>test_MaxPool_f1</td><td>0.50066</td></tr><tr><td>test_MaxPool_loss</td><td>0.709</td></tr><tr><td>test_MaxPool_precision</td><td>0.52569</td></tr><tr><td>test_MaxPool_recall</td><td>0.51608</td></tr><tr><td>test_MaxPool_roc_auc</td><td>0.52371</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">revived-wave-9</strong> at: <a href='https://wandb.ai/mhmdrdaqnbry2025-meybod-university/dreaddit-stress-detection/runs/o1nprw4x' target=\"_blank\">https://wandb.ai/mhmdrdaqnbry2025-meybod-university/dreaddit-stress-detection/runs/o1nprw4x</a><br> View project at: <a href='https://wandb.ai/mhmdrdaqnbry2025-meybod-university/dreaddit-stress-detection' target=\"_blank\">https://wandb.ai/mhmdrdaqnbry2025-meybod-university/dreaddit-stress-detection</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250603_064015-o1nprw4x/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250603_064251-n54sm4u1</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mhmdrdaqnbry2025-meybod-university/dreaddit-stress-detection/runs/n54sm4u1' target=\"_blank\">fancy-dust-10</a></strong> to <a href='https://wandb.ai/mhmdrdaqnbry2025-meybod-university/dreaddit-stress-detection' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mhmdrdaqnbry2025-meybod-university/dreaddit-stress-detection' target=\"_blank\">https://wandb.ai/mhmdrdaqnbry2025-meybod-university/dreaddit-stress-detection</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mhmdrdaqnbry2025-meybod-university/dreaddit-stress-detection/runs/n54sm4u1' target=\"_blank\">https://wandb.ai/mhmdrdaqnbry2025-meybod-university/dreaddit-stress-detection/runs/n54sm4u1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3d1e2586-10ff-4a6b-8a38-764be2155585\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3d1e2586-10ff-4a6b-8a38-764be2155585\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving dreaddit-train.csv to dreaddit-train (10).csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-993caa3e-f4d4-4469-9c6e-e6da23e19de1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-993caa3e-f4d4-4469-9c6e-e6da23e19de1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving dreaddit-test (3).csv to dreaddit-test (3) (10).csv\n",
            "\n",
            "Training Fold 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 1 Epoch 1: 100%|██████████| 837/837 [01:03<00:00, 13.16it/s]\n",
            "Fold 1 Epoch 2: 100%|██████████| 837/837 [01:03<00:00, 13.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Fold 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 2 Epoch 1: 100%|██████████| 837/837 [01:03<00:00, 13.23it/s]\n",
            "Fold 2 Epoch 2: 100%|██████████| 837/837 [01:03<00:00, 13.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Fold 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 3 Epoch 1: 100%|██████████| 837/837 [01:03<00:00, 13.17it/s]\n",
            "Fold 3 Epoch 2: 100%|██████████| 837/837 [01:04<00:00, 13.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Fold 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 4 Epoch 1: 100%|██████████| 837/837 [01:03<00:00, 13.12it/s]\n",
            "Fold 4 Epoch 2: 100%|██████████| 837/837 [01:03<00:00, 13.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Fold 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 5 Epoch 1: 100%|██████████| 837/837 [01:04<00:00, 12.92it/s]\n",
            "Fold 5 Epoch 2: 100%|██████████| 837/837 [01:03<00:00, 13.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Cross-Validation Results:\n",
            "loss:\n",
            "  Mean: 0.2459\n",
            "  Std:  0.0742\n",
            "f1:\n",
            "  Mean: 0.9023\n",
            "  Std:  0.0398\n",
            "accuracy:\n",
            "  Mean: 0.9024\n",
            "  Std:  0.0398\n",
            "precision:\n",
            "  Mean: 0.9037\n",
            "  Std:  0.0395\n",
            "recall:\n",
            "  Mean: 0.9024\n",
            "  Std:  0.0398\n",
            "roc_auc:\n",
            "  Mean: 0.9614\n",
            "  Std:  0.0269\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Test Set: 100%|██████████| 90/90 [00:01<00:00, 46.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Fold 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 1 Epoch 1: 100%|██████████| 837/837 [01:03<00:00, 13.19it/s]\n",
            "Fold 1 Epoch 2: 100%|██████████| 837/837 [01:02<00:00, 13.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Fold 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 2 Epoch 1: 100%|██████████| 837/837 [01:03<00:00, 13.18it/s]\n",
            "Fold 2 Epoch 2: 100%|██████████| 837/837 [01:03<00:00, 13.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Fold 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 3 Epoch 1: 100%|██████████| 837/837 [01:03<00:00, 13.27it/s]\n",
            "Fold 3 Epoch 2: 100%|██████████| 837/837 [01:04<00:00, 13.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Fold 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 4 Epoch 1: 100%|██████████| 837/837 [01:03<00:00, 13.11it/s]\n",
            "Fold 4 Epoch 2: 100%|██████████| 837/837 [01:03<00:00, 13.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Fold 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 5 Epoch 1: 100%|██████████| 837/837 [01:03<00:00, 13.18it/s]\n",
            "Fold 5 Epoch 2: 100%|██████████| 837/837 [01:04<00:00, 12.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Cross-Validation Results:\n",
            "loss:\n",
            "  Mean: 0.2801\n",
            "  Std:  0.0616\n",
            "f1:\n",
            "  Mean: 0.8899\n",
            "  Std:  0.0258\n",
            "accuracy:\n",
            "  Mean: 0.8905\n",
            "  Std:  0.0250\n",
            "precision:\n",
            "  Mean: 0.8958\n",
            "  Std:  0.0212\n",
            "recall:\n",
            "  Mean: 0.8905\n",
            "  Std:  0.0250\n",
            "roc_auc:\n",
            "  Mean: 0.9614\n",
            "  Std:  0.0129\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Test Set: 100%|██████████| 90/90 [00:01<00:00, 47.78it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_GlobalPool_accuracy</td><td>▁</td></tr><tr><td>test_GlobalPool_f1</td><td>▁</td></tr><tr><td>test_GlobalPool_loss</td><td>▁</td></tr><tr><td>test_GlobalPool_precision</td><td>▁</td></tr><tr><td>test_GlobalPool_recall</td><td>▁</td></tr><tr><td>test_GlobalPool_roc_auc</td><td>▁</td></tr><tr><td>test_MaxPool_accuracy</td><td>▁</td></tr><tr><td>test_MaxPool_f1</td><td>▁</td></tr><tr><td>test_MaxPool_loss</td><td>▁</td></tr><tr><td>test_MaxPool_precision</td><td>▁</td></tr><tr><td>test_MaxPool_recall</td><td>▁</td></tr><tr><td>test_MaxPool_roc_auc</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_GlobalPool_accuracy</td><td>0.76783</td></tr><tr><td>test_GlobalPool_f1</td><td>0.76785</td></tr><tr><td>test_GlobalPool_loss</td><td>0.53333</td></tr><tr><td>test_GlobalPool_precision</td><td>0.76788</td></tr><tr><td>test_GlobalPool_recall</td><td>0.76783</td></tr><tr><td>test_GlobalPool_roc_auc</td><td>0.85991</td></tr><tr><td>test_MaxPool_accuracy</td><td>0.77063</td></tr><tr><td>test_MaxPool_f1</td><td>0.77006</td></tr><tr><td>test_MaxPool_loss</td><td>0.59308</td></tr><tr><td>test_MaxPool_precision</td><td>0.77153</td></tr><tr><td>test_MaxPool_recall</td><td>0.77063</td></tr><tr><td>test_MaxPool_roc_auc</td><td>0.8606</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fancy-dust-10</strong> at: <a href='https://wandb.ai/mhmdrdaqnbry2025-meybod-university/dreaddit-stress-detection/runs/n54sm4u1' target=\"_blank\">https://wandb.ai/mhmdrdaqnbry2025-meybod-university/dreaddit-stress-detection/runs/n54sm4u1</a><br> View project at: <a href='https://wandb.ai/mhmdrdaqnbry2025-meybod-university/dreaddit-stress-detection' target=\"_blank\">https://wandb.ai/mhmdrdaqnbry2025-meybod-university/dreaddit-stress-detection</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250603_064251-n54sm4u1/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250603_070928-gnw7nwrf</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mhmdrdaqnbry2025-meybod-university/anxiety-detection/runs/gnw7nwrf' target=\"_blank\">smart-bee-5</a></strong> to <a href='https://wandb.ai/mhmdrdaqnbry2025-meybod-university/anxiety-detection' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mhmdrdaqnbry2025-meybod-university/anxiety-detection' target=\"_blank\">https://wandb.ai/mhmdrdaqnbry2025-meybod-university/anxiety-detection</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mhmdrdaqnbry2025-meybod-university/anxiety-detection/runs/gnw7nwrf' target=\"_blank\">https://wandb.ai/mhmdrdaqnbry2025-meybod-university/anxiety-detection/runs/gnw7nwrf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 Training:   0%|          | 0/1047 [00:00<?, ?it/s]W0603 07:10:01.400000 275 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "Epoch 1 Training: 100%|██████████| 1047/1047 [01:48<00:00,  9.64it/s, loss=0.00709]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 1 Training: 100%|██████████| 1047/1047 [01:36<00:00, 10.84it/s, loss=0.0379]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MaxPool - Val Loss: 0.6730, Val F1: 0.7534\n",
            "GlobalPool - Val Loss: 0.6654, Val F1: 0.7522\n",
            "\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 Training: 100%|██████████| 1047/1047 [00:50<00:00, 20.73it/s, loss=0.0105]\n",
            "Epoch 2 Training: 100%|██████████| 1047/1047 [00:51<00:00, 20.36it/s, loss=0.0409]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MaxPool - Val Loss: 0.8383, Val F1: 0.7393\n",
            "GlobalPool - Val Loss: 0.9152, Val F1: 0.7267\n",
            "\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 Training: 100%|██████████| 1047/1047 [00:51<00:00, 20.45it/s, loss=0.00286]\n",
            "Epoch 3 Training: 100%|██████████| 1047/1047 [00:52<00:00, 20.08it/s, loss=0.0123]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MaxPool - Val Loss: 0.9321, Val F1: 0.7422\n",
            "GlobalPool - Val Loss: 0.9557, Val F1: 0.7315\n",
            "\n",
            "Epoch 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 Training: 100%|██████████| 1047/1047 [00:51<00:00, 20.44it/s, loss=0.00229]\n",
            "Epoch 4 Training: 100%|██████████| 1047/1047 [00:51<00:00, 20.27it/s, loss=0.00794]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🛑 Early stopping triggered. Training stopped.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8AjJrsionNge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kMKeolKFE3Nn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}